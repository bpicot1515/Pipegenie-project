0: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(66, 10),
             learning_rate_init=0.04047224825003958, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19524145151700953

1: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(66, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19524145151700953

2: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(66, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19524145151700953

3: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.3669659819639991,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.19939755716168603

4: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.2594997151481754,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.19939755716168603

5: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.03384354003355651, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

6: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.3991018719997404,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.19939755716168603

7: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.03384354003355651, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

8: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.3991018719997404,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.19939755716168603

9: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.3991018719997404,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.19939755716168603

10: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.20482938851269247, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

11: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.04047224825003958, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

12: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.04047224825003958, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

13: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.04047224825003958, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

14: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

15: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

16: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.09547520202582262,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.19939755716168603

17: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.08867600838780476,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.04047224825003958, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.20058463949202596

18: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.08867600838780476,
             hidden_layer_sizes=(47, 10),
             learning_rate_init=0.48306811135064043, random_state=42,
             solver='lbfgs')
) -> Fitness: 0.20058463949202596

19: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='tanh', alpha=0.08867600838780476,
             hidden_layer_sizes=(47, 10), learning_rate_init=0.3991018719997404,
             random_state=42, solver='lbfgs')
) -> Fitness: 0.20058463949202596

Ensemble fitness: 0.1956321070750683
Weights: [1.0, 1.0, 1.0, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9791566872541652, 0.9733619284679631, 0.9733619284679631, 0.9733619284679631]
Prediction: [ 3.61662914e-01 -2.76821973e-01  1.51208797e+00  1.11045927e+00
  4.89141261e-01 -1.15772224e+00  9.92223211e-01 -1.03972729e+00
  6.22350666e-01 -8.22991830e-01  5.62524990e-01  1.31101639e+00
 -1.23293776e+00 -9.67793095e-02  2.02210520e+00 -1.66900423e+00
 -2.28517510e-01  1.48794026e+00  1.25462474e-01  1.35746210e+00
 -7.05308641e-01 -1.51111813e+00 -2.76279409e-01  4.77333394e-01
  6.93764543e-01  6.01009901e-01  3.87426136e-01  9.76226154e-01
  4.33278188e-02 -1.73353245e+00  9.57117720e-01  8.20163233e-01
 -4.69797996e-01 -2.89092360e-03 -5.92515574e-01  5.63744637e-01
  3.34606603e-01 -9.69489733e-01 -1.63263531e+00  2.68490207e-01
 -5.44468029e-01 -9.84598351e-01 -6.48308736e-01  6.29144635e-01
 -1.00445532e+00 -1.27047158e-02  3.09336067e-01 -3.49620895e-01
  5.31036166e-01  2.68818224e+00 -1.44463645e+00  4.88040034e-01
  1.83172107e-01 -3.80184886e-01  3.39210583e-01  1.17607242e+00
 -6.17422560e-01 -7.60776691e-01 -1.69803173e-01  1.05466711e+00
  2.14913159e-01  1.10890051e+00  3.68473805e-01  7.28327400e-02
 -4.15450748e-01  5.12034102e-01  3.26241523e-01 -1.01053170e+00
  1.44377031e+00 -1.73941767e+00 -6.34823375e-01 -1.29284997e+00
  8.26777352e-02 -1.14474314e+00  1.31496965e-01  5.00450771e-01
 -1.12805735e+00 -3.19867505e-01  5.58074957e-01  4.35137967e-01
  2.66652411e-01  1.01822390e+00 -9.83657442e-02  1.28782804e+00
  1.67554286e+00 -3.35718739e-02 -9.23128062e-01 -1.44097875e+00
  5.30495858e-01 -2.34748081e-02  6.08767160e-01 -8.08751732e-02
 -2.17910497e+00  1.39199541e+00  3.56397260e-01  2.50569691e-01
 -8.27383880e-01  1.96626745e-01 -1.36517950e+00  7.31681597e-01
  4.71727510e-01 -2.30585858e-01 -4.83599094e-01  2.35070554e+00
  2.71482834e-01 -5.72743912e-01  1.51572182e-02 -1.33469426e+00
 -1.79617065e-01  3.08644781e-01  1.19265469e+00  1.57655569e+00
  4.12379156e-01 -3.41599990e-01  4.96274288e-02  4.70101459e-01
  6.26171907e-01 -1.85786022e-01 -7.94058341e-01  3.76856930e-01
 -8.89937196e-01 -4.13889761e-01 -6.00293065e-01 -2.29456151e+00
  1.79592847e+00  5.29666914e-01  9.02542101e-01  5.89235234e-01
  1.02247820e+00 -7.04075177e-01  7.16606699e-01  3.08279001e-01
  1.35118953e+00 -1.00751264e+00  4.75932861e-01  3.55266718e-01
  2.38120632e-01  1.42485636e+00  1.61649469e-01  5.05424087e-01
 -6.48095157e-01 -1.41204789e-01 -1.06202452e+00  1.40460745e+00
  2.31192722e-01  4.44982979e-01  2.21031633e-01 -9.13599829e-01
 -2.76169496e-01  1.09980410e+00 -1.08111109e+00 -3.33845116e-01
  6.86739164e-01  8.81884329e-01  5.49721338e-01 -1.22940990e+00
 -1.57108775e+00  6.84527331e-01  1.01383029e+00  6.26266146e-01
 -1.88059019e-01  3.13288070e+00  1.16290713e+00  3.41679910e+00
  4.09938822e-01  6.21519864e-01 -7.31805076e-01 -1.85341863e-01
 -5.18647759e-01 -2.13425225e+00 -6.08665499e-01 -1.89310032e+00
 -1.98071855e-01 -2.01317114e+00 -6.53641034e-01 -1.26251874e-01
 -2.32496154e-01 -1.93453873e+00 -6.89376409e-01  6.34442256e-01
  1.36696184e+00 -5.90939767e-01 -6.97497832e-01  1.15952534e+00
  8.54163184e-01  1.86793491e-02 -3.67713294e-01  8.52580260e-01
 -1.82276406e-01 -5.51154002e-03  9.15052834e-01 -2.19144759e+00
 -9.89659531e-01  1.04146199e-01  6.63712414e-01  6.39079798e-01
  8.28127645e-01  1.14798647e+00  2.96128679e+00  2.18613290e-01
  5.95841267e-01 -6.45992078e-01  8.31038304e-01 -3.99511778e-01
 -1.02701175e-01  4.90309599e-01  5.95111558e-01 -3.69865010e-01
  5.05948605e-01  2.96859485e-01  3.74422666e-01 -1.89482580e+00
 -1.79539442e+00  9.71944580e-02 -1.92321014e+00  7.12402055e-01
 -3.78544454e-01 -1.70057357e-01  1.71351765e-01  1.37761544e+00
 -8.12406035e-02 -5.71173845e-01  1.03431156e+00  1.26772777e+00
  5.85372977e-01 -8.23503756e-01 -8.38243694e-01  4.28662531e-01
 -9.88139623e-01 -1.78311591e-01 -2.77184335e-01  3.08973236e-01
  1.65566384e+00 -1.06643140e+00  1.73326983e-01  1.81602480e+00
  4.05544077e-01 -1.08156242e+00 -1.44352184e+00  8.08443444e-01
 -1.58689748e+00 -5.32859671e-01 -7.91570991e-01  1.32914756e-01
  1.20443241e+00  8.03600802e-01 -5.06737136e-01 -3.88679325e-01
  1.68012589e-01  6.65083835e-01 -1.40904010e+00 -5.13732531e-03
 -1.95961864e+00  3.53574510e+00 -5.36753359e-01 -5.19132797e-01
 -4.41017709e-01 -7.10326940e-01  8.94706025e-01  3.55599588e-01
  9.03068559e-01 -2.63960666e-01  9.36324226e-01 -8.36803858e-01
  3.53699815e-01  7.46972643e-01  1.59531967e+00 -3.39278912e-01
 -2.05087134e+00 -5.26570785e-03  8.34052493e-01  4.76130974e-01
  9.84632557e-01 -1.54783393e+00 -1.84699492e+00 -8.69918425e-02
 -1.51409679e+00  5.15265374e-01  3.46545717e-01  1.04578563e+00
  1.48166534e+00 -1.39070438e+00  6.87693116e-01  1.62353076e+00
 -1.56226227e-01  3.05561652e-02  3.30309808e-02  2.06836981e-01
 -6.90722357e-03  2.11297026e-01  6.59818118e-01 -7.54248946e-01
  4.43546237e-01  1.90468119e-01  1.02149150e+00 -1.37792058e+00
 -1.89965720e+00  5.62931380e-01  8.25036455e-01  2.39762219e+00
 -1.75622355e+00  2.88817112e-01 -2.83181262e-01  9.22197603e-02
 -1.96062513e+00 -1.23131008e-01 -2.58567119e-02  8.28616121e-01
 -9.26558415e-01 -8.24879480e-01 -2.10762435e+00  3.73938620e-01
  3.04755484e-02  5.01753061e-01  3.29829123e-01  8.42412776e-01
  1.81534040e-01 -2.20014774e+00 -3.97608163e-01 -1.27748178e+00
  1.26288062e+00  5.59870270e-01  1.46157931e-02 -1.26213553e+00
  2.90108477e-01  4.49776185e-01  1.03694460e+00 -7.74311157e-01
  8.53244324e-01  4.38076752e-01 -1.04833446e-01 -4.91392159e-01
 -1.21660657e+00  5.17598389e-01  6.12247079e-01]
