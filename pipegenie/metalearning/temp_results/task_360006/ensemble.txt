0: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.048255286531407655, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5864586947063293

1: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.048255286531407655, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5864586947063293

2: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.048255286531407655, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5864586947063293

3: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.04904588904777868, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5870059125262104

4: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.04904588904777868, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5870059125262104

5: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.04904588904777868, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5870059125262104

6: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.04252276588391275, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5873816817132738

7: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.07396010383320115, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.587455072042152

8: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.04832088159285867, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5875199883328464

9: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.034890153029573386, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.587532550821933

10: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.09765210454436692, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5875659060320975

11: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.09765210454436692, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5875659060320975

12: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.030306002573703392, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.587730541782126

13: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.030306002573703392, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.587730541782126

14: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.030306002573703392, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.587730541782126

15: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.03854405799275531, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5877441081732648

16: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.09367994889266541, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5880209634008853

17: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.09367994889266541, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5880209634008853

18: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.09367994889266541, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5880209634008853

19: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.0767161968206871, hidden_layer_sizes=(46, 14),
             learning_rate_init=0.005922903140295294, random_state=42,
             solver='sgd')
) -> Fitness: 1.5880530420229877

Ensemble fitness: 1.5873676705050126
Weights: [1.0, 1.0, 1.0, 0.9996551885436835, 0.9996551885436835, 0.9996551885436835, 0.9994185475254141, 0.9993723429699708, 0.9993314769991453, 0.9993235690726168, 0.999302573000868, 0.999302573000868, 0.9991989528182981, 0.9991989528182981, 0.9991989528182981, 0.9991904152184734, 0.9990162165799057, 0.9990162165799057, 0.9990162165799057, 0.9989960364833738]
Prediction: [ 7.34475614 13.77468982  6.91395826  9.57722994  7.30817371  6.79947043
  7.6203444   7.26240765  7.29646535  6.02327472 12.41775512 14.9242642
  7.49027994  7.11368362  9.04492366 11.60317421  6.45280783  6.97788319
  7.31919833  7.75796956  8.84647103  7.53516573  7.10514646  6.93861511
 11.54997371  6.92156412  6.66370586  7.32551636  6.77804411  7.92041486
  7.77659688  6.54121498  7.76044379 10.62502528  8.60586666  6.73466209
  7.26466651  8.12166511  7.39566467  7.92049798 10.35437346  6.76635878
  6.9724251   9.65703864  7.46282362  8.0015699   5.95079649  9.60238328
  8.62145811  6.99580335  7.00972254  8.82557004  7.47220697  6.75394805
  9.18270923  7.35889255 10.17730551 10.207945    6.70180016  7.71525492
 15.80730756  7.03495848  7.0343383   7.59614139  9.5063918   7.66761945
  6.48457745  7.21940783  6.75566618  6.295512   10.51261644  6.41836191
  6.26496607 10.13167314  6.58612852 25.01846996  6.67607859  6.90914389
  6.42874051  6.39675516  8.71699798  9.1144247  14.90874075  6.28561865
  6.15686055  6.84639701  9.32934221  6.88283561  6.73799318  6.87640076
  7.63792433 11.23711683  8.03722712  7.06117148  7.48509368  6.92605741
  7.23722955  8.17142239 11.74408714  8.77232035  8.66968567  7.53652957
  7.10699298  6.14240691  9.03993403 13.84533566  6.59226262 11.42227983
  7.57605514  5.87829383  8.10355703  8.15372626  7.48674336  6.78737647
 13.16596342  6.62677803  8.25974201  7.09131317  9.28912004  8.50696883
  7.52292634 11.38377161  8.30938106  6.91471077  8.05685931  8.26555886
  8.40634291  7.44207984  9.16209938  7.09003343 12.9134873   7.12747329
  6.22946563  9.31914876  8.0007683   7.74688718  8.77933595 11.89053935
  8.70243863  6.71795505 10.28697551  8.39659262  7.76709462 17.9758576
  8.39321884  9.13702652  9.08682435  8.24524986  9.03941816  7.02565653
  7.44368006  8.35137788  6.94548044  6.14590565  7.97033465  9.63453286
 15.75447654  6.90234249 11.68217626  6.42569399  7.08054959  8.50717206
  6.61128327  9.4895686   7.89309444  8.95207163  7.72094561  8.09924806
  7.36967503  7.69247731  8.56203274 16.05523628  6.83365519  7.01967786
 10.142875    8.71521639  9.36651482  8.93886511  8.33025769  7.72695162
  8.17097372  8.26327661  7.07248999  8.04588203 12.31713417  6.7508187
  7.96132002  6.09186782  9.37672278  7.34021747  8.25525571  6.71222471
  7.39005585  9.03175149  6.24277316  7.06837577  7.61349977  7.15679419
 11.97820236  7.72033483  7.00430121  9.06382468 12.62037003  7.89821784
  8.0296592   7.72004431  9.51194923  7.08045362  6.83284483  6.23907351
  7.62051582  7.42294301 10.27987403  7.2548664   7.48088334 14.5510827
  8.81860448 13.13270605  6.98115319  7.40115546  7.53231351  9.91032245
  8.32843827  6.77848079  7.09782683  8.55165599  7.62093439 11.37810036
  6.24574748  7.52586183  8.72976338  9.43564942  8.48767242  7.01962727
  6.63960947 14.44150791  7.1364522   8.15667597 10.51213517  9.30122862
  6.787151    8.73561709 10.89979044  8.92561199  8.08290224  6.87614247
  9.97217259  7.3013361  14.19846745  7.18803474  9.20035358  8.04693414
  7.91832168 12.45491687  8.82471811  7.2344521  11.30047354  8.26775291
  7.20119196 19.19519346  8.25020705 10.53157027  6.34991539  7.99256769
  8.9304142   7.14769357  7.55552164 10.10706253  6.50008354  7.0267755
  6.66041336  6.66338577  6.63815031  7.86064722  8.13349959  6.45681553
  6.39762464  6.49399003  7.07264079  8.5188869  17.73453165 11.2276654
  9.21207341  7.53367046  7.24396023  6.69151297  6.31513913  6.8071555
 15.25964029  6.43417502  9.70494524  8.24493505 13.54585436  7.28461316
  7.30071332  6.99796474  6.55014258 11.53503361  7.27078535  7.30639359
  8.13636529  7.73868082  7.35332581  9.1740903   8.54783957 13.06548233
  7.36583436  7.10641968  6.76677464  7.28494611  6.87702866  7.43890306
  6.60933057  8.17601858  7.14650345  6.88496231 10.40666751  7.03139785
  8.12782016  7.85123906  8.12181274  7.49762264  7.28624709  7.24773739
 16.28515129  6.6092426   7.60984265  6.78524272  6.6962996   7.47791722
  6.64898106  6.73368518  6.85960938  7.1195539   7.20985257  7.79163281
  7.06085807  9.06198243  9.7364549   7.72666419  8.14380081]
