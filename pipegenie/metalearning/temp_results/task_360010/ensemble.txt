0: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(25, 2),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 1.5545282691363962

1: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.0019498771279302645, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.266815263568521

2: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.0019498771279302645, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.266815263568521

3: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.8705335086248875, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.4092376051754574

4: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.8705335086248875, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.4092376051754574

5: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.4092376051754574

6: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.4092376051754574

7: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.494580875589506

8: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.494580875589506

9: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(57, 13),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.494580875589506

10: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: MLPRegressor(alpha=0.07482344388447094, hidden_layer_sizes=(62, 8),
             learning_rate_init=0.027286791778978237, random_state=42,
             solver='lbfgs')
) -> Fitness: 2.829387199488346

11: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: MLPRegressor(alpha=0.060714376955917276, hidden_layer_sizes=(25, 2),
             learning_rate_init=0.335650391846521, random_state=42,
             solver='lbfgs')
) -> Fitness: 3.24177391070283

12: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: MLPRegressor(activation='identity', alpha=0.060714376955917276,
             hidden_layer_sizes=(57, 13), learning_rate_init=0.8705335086248875,
             random_state=42, solver='lbfgs')
) -> Fitness: 3.3935964151431373

13: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: MLPRegressor(activation='identity', alpha=0.060714376955917276,
             hidden_layer_sizes=(57, 13), learning_rate_init=0.8705335086248875,
             random_state=42, solver='lbfgs')
) -> Fitness: 3.3935964151431373

14: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MaxAbsScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018496

15: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MaxAbsScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018496

16: Pipeline(
	0: SimpleImputer()
	1: MaxAbsScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018496

17: Pipeline(
	0: SimpleImputer(strategy='median')
	1: MinMaxScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018514

18: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: MinMaxScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018514

19: Pipeline(
	0: SimpleImputer()
	1: MinMaxScaler()
	2: LinearRegression()
) -> Fitness: 3.3994575961018514

Ensemble fitness: 2.0321106486652587
Weights: [1.0, 0.6857763374546847, 0.6857763374546847, 0.645236595094233, 0.645236595094233, 0.645236595094233, 0.645236595094233, 0.6231621048441809, 0.6231621048441809, 0.6231621048441809, 0.5494222457136692, 0.4795301313284269, 0.4580769422668159, 0.4580769422668159, 0.45728714807884946, 0.45728714807884946, 0.45728714807884946, 0.45728714807884924, 0.45728714807884924, 0.45728714807884924]
Prediction: [ 3.11193154e-01  4.19067617e-02  1.73814752e-01  2.03097701e-01
  6.39677995e-01 -6.95679701e-02  1.03439873e-01  8.35244744e-02
  1.09039756e+00  1.61693072e-01  4.16451492e-01  1.56422443e-01
  1.16500459e-01  1.02043301e-01  3.36599268e-01  2.63441297e-01
  8.64870410e-03  4.64343854e-01  2.87043564e-02  5.69254364e-02
  3.79620356e-02  1.14723705e+02  3.53893746e-02  1.56530686e-01
 -7.10841581e-02  4.35480198e-01 -9.91997971e-02  5.29120332e-02
  3.43718139e-02  4.67845920e-01  3.03151769e-02  7.07258758e-02
  7.37619499e-01  9.35048912e-02  1.47164205e-01  4.87015052e-01
  3.35245390e-02  8.17752652e-02  7.26757588e-02  9.87558383e-01
  7.29081337e-02 -4.28273081e-02 -7.33842673e-04  6.18076662e-02
  6.91706580e-02  5.76904978e-01  1.02742450e-01  2.61498256e-01
  4.65224630e-03  9.24880684e-01  1.31643516e+01  3.05697822e-01
 -1.76325244e-03  2.28287314e-01  6.53029586e-01  3.17486118e-01
  3.89983354e-03  4.37493394e-01  8.23176032e-02  6.24023354e-01
  5.33810497e-01  1.78470467e-01  6.09593273e-01 -7.01674095e-02
  2.57676598e-01 -3.78588395e-02  8.03518793e-01 -5.82107334e-02
  2.36101252e-01  1.96277813e-01  5.52827916e-01  1.59689403e-03
  4.65873237e-02  1.01839286e-01  3.76102490e-01  1.31221409e-01
  1.35595903e-02  4.52071873e-02  4.67998689e-02  5.54287091e-01
  2.14083995e-02  5.50033631e-01  3.95277538e-01  3.20179386e-01
  3.21772486e-01  4.80913442e-02  4.35912521e-02  2.22576842e-01
  8.03007322e-01  5.50881724e-01  1.73045390e-01  1.01766811e-01
  3.93115270e-01  8.06158448e-02  9.39774286e-01  4.09160022e-01
  4.73162761e-02 -5.59362525e-02  2.26966826e-01  9.86159793e-02
  1.02838109e-01  8.36462354e-02 -3.60581720e-02  3.12386779e-01
  1.71650465e-01  9.21233688e-01  5.69100003e-01  6.40095753e-01
  3.77671520e-02  3.08069132e-01  8.27434579e-02  1.68309073e-02
  2.33557696e-01 -3.83206262e-02  1.07346941e+00  7.13632129e-01
  8.28580319e-01  1.05264835e-02  1.86322464e-01  3.62089195e-04
  1.20334662e-01  2.70841740e-01  5.45383685e-02  2.58238663e-01
  9.89462259e-02  2.37116780e-01  2.19025016e-01 -4.93220975e-03
  5.84405752e-02  4.96228867e-01  5.61352971e-03 -2.39367166e-02
  2.22164745e-02  1.40655306e-02  1.38863063e+00  6.39479172e-02
 -2.69422772e-02 -1.01859887e-03 -9.21302847e-02  3.56472985e-02
 -9.43075753e-02  2.14670170e-01  5.29791863e-01  3.90643266e-01
  2.85655422e-01  3.55675930e-01  1.25296453e-01  6.46171978e-01
  2.16634032e-01  4.53730743e-01  4.96984822e-02  3.45986231e-02
  1.58641596e-02  1.08585163e-01  8.41960326e-01  3.20793253e-01
  1.76848196e-01  1.65926138e-01  1.53485157e-01  3.70907445e-01
  1.79096657e-01  8.59650571e-02  6.97145970e-02  1.86074773e-02
 -2.24971003e-02  1.46710900e-01  6.06348902e-02  1.13344417e-01
  8.66233530e-02  1.31332950e-01 -3.31561476e-02  1.69858063e-01
  3.20680488e-01  1.35063507e-01  1.25079771e-01  7.01527962e-01
  7.25587351e-01  2.74579464e-01  3.00545083e-01  5.78671161e-02
  5.25008381e-02  1.12887582e-01  4.64324119e-02  1.25926210e-01
  2.15641176e-01  2.88737292e-01  8.53057309e+01  2.30483435e-02
  7.58808376e-02 -2.74806075e-03  1.06508899e-01  2.76771143e-01
  1.79568066e-01  5.08469038e-02  1.44128555e-01  2.13079650e-01
  6.36453089e-02  1.63004634e-02  2.21443159e-01 -1.17316503e-01
  8.03989102e-01  3.30887033e-01 -6.35270030e-02  5.72451407e-01
  5.08629974e-02  7.62729041e-02  8.46142732e-01  1.55082846e+02
  3.47067931e-01  1.53393681e-02  1.57429007e-01 -1.38773405e-01
  1.42462394e-01  2.67378561e-01  3.45856778e-02  3.92564099e-01
  7.24046960e-01  2.93448929e-01 -1.68294499e-02  1.47015281e-01
  9.52172706e-01  5.58474381e-02  6.48047524e-02  4.70127370e-02
  3.43536495e-01  1.25621601e-01  1.12920150e-01  2.25025296e-01
  2.09310060e-02 -1.46388056e-02  2.37672767e-01  2.32647726e-01
  4.92794217e-01  1.24296896e-01  1.02637050e+00  2.56031508e-01
  4.35151091e-02  5.72895528e-02  6.31568592e-01  1.18683822e-01
  6.53064862e-01 -4.67638084e-02 -4.74836052e-02  1.43689237e-01
 -1.34316398e-02  1.60045650e-01  1.10196215e-01  2.03806962e-01
  1.92533686e-01  4.22798624e-02 -1.08276667e-01  4.02894513e-01
  1.75152174e-01  2.63809716e-01  1.48932189e-01  3.66158321e-02
  1.68507533e-02  4.08747376e-01  5.85117829e-01  1.46938250e-01
  9.27241115e-02  2.26151171e-01  4.44872174e-01  6.80648217e-02
 -2.76900340e-02  1.23882668e-01  2.35368826e-01  3.64640191e-02
  2.89364349e-01  4.23191784e-02  3.74695184e-01  6.47837804e-01
  3.60054330e-01  6.04652058e-01  6.27341865e-02  7.54756208e-02
  2.59302144e-02  1.04788335e-01  2.24656648e-01  6.69520306e-02
  1.75478286e-01  4.07153719e-01 -2.14764411e-03  5.18574539e-01
  2.97376565e-01  1.05111091e+00  2.79888939e-01  3.34677759e-01
  2.56824898e-01  6.73574017e-02  2.48983780e-01  3.00497911e-01
  2.20364086e-01  2.89583724e-01  2.26588771e+02  6.63255332e-01
  8.19235769e-02 -3.85466992e-02  4.48655433e-01  6.40677488e-02
  1.01942739e-01  1.64869376e-01  9.72629722e-02  2.98882192e-01
  1.34732561e-02  1.84206101e-01  1.67880410e-01  2.33629241e-01
  2.41882050e-05 -8.46669548e-03 -2.97671073e-02 -5.22174326e-02
  1.76924569e-01  1.34061815e-01  1.78752048e-01 -5.47869613e-02
 -3.95467028e-02  1.33074165e-01  1.92048868e-02  9.11196640e-02
  9.36603819e-02  2.29787060e-02  1.72473634e-01  7.21037011e-02
  5.04799566e-01  6.15445530e-01  6.35811231e-01  4.17670020e-01
  9.54292741e-02  1.83530124e-02  2.71779704e-02  2.35417109e-01
  1.88051142e-01  2.69185331e-01  2.45736789e-01  8.74689213e-02
  1.40015284e-01  7.25591060e-01  2.44801150e-01  4.34122799e-02
  8.87230137e-02  6.08271592e-01  5.66965864e-01  1.73373286e-03
  5.26863150e-01  4.99143461e-01  3.63635062e-01  1.66315070e-01
 -1.02907675e-02  1.59946749e-01 -8.43510795e-02  1.22457640e-01
  6.69424637e-02 -2.79010967e-02  5.98777670e-02  4.14425700e-01
  1.43399346e-01  2.99270460e-02 -8.49097875e-02  5.32185408e-02
  3.55023300e-01  1.81834645e-02  4.79471823e-01  1.93570290e-01
  3.55578681e-01  7.55582109e-01  6.59911306e-03  3.90034975e-01
  2.51319451e-01  5.70262177e-03  3.15846160e-01  2.09160990e-01
  8.28305046e-01  8.52829410e-02  3.59258149e-01]
