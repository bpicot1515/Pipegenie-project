0: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.06617456605842978,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

1: Pipeline(
	0: SimpleImputer()
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.06617456605842978,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

2: Pipeline(
	0: SimpleImputer(strategy='median')
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.36016676872091785,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

3: Pipeline(
	0: SimpleImputer()
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.36016676872091785,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

4: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.48306811135064043,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

5: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.36016676872091785,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

6: Pipeline(
	0: SimpleImputer()
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.48306811135064043,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

7: Pipeline(
	0: SimpleImputer(strategy='median')
	1: StandardScaler()
	2: MLPRegressor(activation='tanh', alpha=0.0009828064620035553,
             hidden_layer_sizes=(47, 2), learning_rate_init=0.48306811135064043,
             random_state=42, solver='lbfgs')
) -> Fitness: 2.6951451856211284

8: Pipeline(
	0: SimpleImputer(strategy='median')
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

9: Pipeline(
	0: SimpleImputer()
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

10: Pipeline(
	0: SimpleImputer()
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

11: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

12: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

13: Pipeline(
	0: SimpleImputer(strategy='median')
	1: RobustScaler(with_centering=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7796111745684016

14: Pipeline(
	0: SimpleImputer(strategy='median')
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

15: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

16: Pipeline(
	0: SimpleImputer(strategy='most_frequent')
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

17: Pipeline(
	0: SimpleImputer()
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

18: Pipeline(
	0: SimpleImputer(strategy='median')
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

19: Pipeline(
	0: SimpleImputer()
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: AdaBoostRegressor(estimator=DecisionTreeRegressor(criterion='poisson',
                                                  max_depth=7,
                                                  max_features=0.7843071728708866,
                                                  min_samples_leaf=4,
                                                  min_samples_split=18,
                                                  random_state=42),
                  learning_rate=0.4225081630182586, n_estimators=33,
                  random_state=42)
) -> Fitness: 2.7806348317402336

Ensemble fitness: 2.644292693827847
Weights: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9696123005548111, 0.9696123005548111, 0.9696123005548111, 0.9696123005548111, 0.9696123005548111, 0.9696123005548111, 0.969255349482333, 0.969255349482333, 0.969255349482333, 0.969255349482333, 0.969255349482333, 0.969255349482333]
Prediction: [ 6.71195842e-01  1.84265673e-01 -5.81190741e-02  8.96016103e-02
  1.23139200e+00 -4.09880764e-02  1.57757114e-01  8.87161204e-02
  4.27380922e-01  1.28220451e-01  3.51986230e-01  3.66210222e-01
  1.38501241e-01  4.49546015e-02  1.81791850e-01  3.44501166e-03
  2.98238686e-01  3.59646711e-01  2.40819735e-02  1.55585005e-01
  3.34574826e-01  3.09475248e+01  1.28500117e-01  2.57069307e-01
  9.97813234e-02  1.51907225e-01 -1.12384679e-01 -4.73935140e-03
  2.96307554e-02  3.64565469e-02  2.02989535e-02  4.42704054e-02
  3.72572771e-01  1.96340270e-03  1.47924798e-01  7.22562155e-01
  2.22801504e-02 -3.48314955e-03  1.92395839e-01  5.57545256e-01
  1.42735258e-01  8.74325344e-02 -2.80079561e-02  3.01481505e-01
  2.39742876e-01  4.73631187e-01  7.75895955e-02 -1.00857662e-01
 -9.72388930e-02  2.82596986e-01  2.84490175e+00  5.37867772e-01
  1.06816228e-01  5.14772365e-02  2.11951648e+00  8.49398234e-02
 -1.04001790e-01  3.91020101e-01  1.39175957e-01  3.55468110e-01
  3.15627466e-01  1.80501124e-01  8.54033025e-01 -5.77645364e-02
  5.91942050e-02  1.73276934e-01  3.77501967e-01 -9.56775277e-03
  1.88443801e-01  1.17511988e-01  3.89720043e-01 -1.00423360e-02
  2.45558885e-01  2.62696258e-01  9.77238343e-02  8.87535102e-02
  8.70528077e-02  8.60564649e-02  1.04205469e-01  5.05177181e-01
  2.27490106e-01  4.34673698e-01 -3.90421520e-03 -1.53691012e-02
 -8.89060269e-02  2.14050775e-01  1.67189983e-01  2.67243340e-01
  9.78707230e-02  7.45197531e-01  1.56074668e-01  4.82003958e-01
  9.33243840e-02  5.79757780e-02  1.98476022e-01  2.80091016e-01
  5.32866363e-02 -5.03910985e-02  1.84239960e-01  3.71591498e-02
  2.57102222e-01  1.31950276e-01 -5.43331146e-02  2.33454210e-01
  1.42223030e-01  3.99766852e-01  3.35028821e-01  9.91850057e-01
  7.94142363e-02  2.94638897e-01  2.19921990e-01 -1.51261043e-01
  6.63014249e-01  2.52623736e-02  2.25572363e-01  4.44276663e-01
  3.41336328e-01 -2.56673565e-02  7.91639362e-01  3.02735385e-01
  1.77579984e-01  1.69658415e-01  9.19342216e-02  1.17755660e-02
  2.00955310e-01  4.12985562e-02  5.21523744e-01  2.84118113e-01
 -3.24776049e-02  3.89987430e-01  2.13736816e-01  3.26048559e-02
  6.51585915e-02  1.38320433e-01  1.73987449e+00  1.36610328e-01
 -9.02535615e-03  2.44869395e-01  8.25925872e-02  1.44088319e-01
 -3.32408739e-03  1.67319044e-01  9.10672781e-01  9.76217987e-02
  3.95936944e-01  2.60398904e-01  1.45167635e-01  7.27937740e-02
 -3.02731420e-02  3.39205026e-01  1.96167375e-01 -4.42914043e-02
  8.68217785e-02  4.27127495e-02  4.55806566e-01  1.06579309e-02
  1.52963769e-01  4.84973213e-01  1.58752258e-01  4.11807344e-01
  2.62479835e-01  5.30917243e-02  1.54465306e-01  1.85980007e-01
 -7.87410891e-03  6.25753115e-02  1.82456985e-01  2.71284023e-01
  2.54230313e-01  2.62242445e-01  1.19637375e-01  7.54903014e-02
  9.72289689e-01  1.35802118e-01  8.11056034e-02  2.97752338e-01
  3.96200310e-01  2.36228668e-01  9.76832770e-01  3.12091553e-01
  1.78385976e-02  5.25340693e-02  2.76606597e-01  1.61726389e-01
  4.94482213e-01  1.39415704e-01  2.12036634e+01  9.26190191e-02
  6.14338977e-02  3.07294240e-01  1.56727132e-01  4.86561206e-01
  1.55196568e-01  1.16527219e-01  1.08526234e-01  1.00350942e-01
  1.50889156e-01  5.17641350e-02  8.51717777e-02 -1.77167813e-01
  1.98847388e-01  4.99962483e-01  1.45248314e-01  1.71153645e-01
  3.19159961e-01  1.32064166e-01  4.16810255e-01  5.19289566e+01
  4.09407523e-01  6.56057199e-02  1.63597721e-01 -7.82408354e-02
  1.18345521e-01 -2.21592307e-02  7.04209860e-02  4.23885637e-01
  2.39514466e-01  5.01735992e-01  5.52373929e-02  3.38565074e-01
  2.71759956e-01  6.38262077e-02  7.07938488e-02  8.44766171e-02
 -8.34824861e-03  1.57081489e-01  3.16160936e-01  4.63108881e-01
  1.15559075e-01  2.21165794e-02  8.24289800e-01  2.07657754e-01
  3.27480555e-01  7.09393450e-02  4.09928218e-01  2.61854984e-01
  1.21184528e-02  1.07742357e-01  5.14707661e-01  2.03339483e-01
  3.22458234e-01 -1.08187276e-01 -1.49147782e-02  1.74299154e-01
  1.02587922e-01  5.88941696e-02  2.99464032e-01  9.10038471e-02
  1.61960148e-01  3.71667765e-01  5.03906792e-02  3.44736227e-01
  1.64938850e-02  2.61629413e-01  1.79369287e-01  2.77011945e-01
  8.67526738e-02  5.36107092e-01  3.15970107e-01  1.98521846e-01
  1.85433923e-01  1.07943671e-01  8.72532394e-01  1.18301823e-01
  8.03102693e-02  1.82597737e-02  1.67130555e-01  6.14149594e-02
  7.22846264e-02  3.16661912e-01  4.40602778e-01  2.08152831e-01
  1.02701588e-01  4.64815333e-01  2.19024275e-01  1.51350421e-01
  1.88330978e-01  6.51530615e-02  2.34291224e-01  2.64129078e-01
  9.64057762e-02  4.04245519e-01  1.02814596e-02  2.01557589e-01
  7.89606956e-02  2.04789561e-01  1.54126163e-01  2.14192249e-01
  1.60897685e-01  1.60724731e-01 -5.50050174e-02  3.13288242e-01
  2.47322591e-01  4.95431308e-02  1.26026274e+02  7.66925161e-01
  3.16157538e-01  1.25511828e-01  2.36491807e-01  2.57336859e-01
  1.83028655e-02  2.23823725e-01  1.73388769e-02  3.35902300e-01
  3.36936590e-03  1.62848884e-01  7.58333458e-01  5.58914031e-01
  1.25354839e-01  2.28759627e-01  4.49011272e-02  8.21028672e-02
  8.32568567e-02  8.06277839e-01  5.92266638e-02  2.45083993e-02
  4.12351579e-02  1.34751311e-01  1.26922930e-01  2.71139979e-01
  2.28252092e-01  5.82854969e-01  5.96630237e-02  4.31296768e-02
  5.26394680e-01  4.78985376e-01  4.64563003e-01  1.02228594e-01
  1.43406243e-01  7.43449424e-02  4.32191824e-02  2.26504153e-01
  1.80958503e-01  1.25470657e-01  2.65771837e-01  3.99028454e-02
  1.28635226e-01  2.76194627e-01  7.95718709e-02  2.86024922e-01
  2.47965288e-01  3.47884035e-01  3.64487509e-01  3.99397375e-01
  3.36044346e-01  3.89643727e-01  3.17094249e-01  1.47998399e-01
  6.92737170e-02  9.26170110e-02  2.44272147e-02  2.93429245e-01
  1.35691313e-01  1.66028445e-01  1.62875598e-02  3.68664537e-01
  3.75396729e-02  4.66352860e-01 -8.78882585e-02  1.81232349e-01
  3.28832746e-01  3.38470834e-02  3.70023588e-01  1.25531584e-01
  5.22940714e-01  5.81671566e-01  2.43330521e-01  4.97525194e-01
  2.77661591e-01  3.59936839e-02  3.85766380e-01  1.06674122e-01
  1.86543469e-01  3.20245432e-02  1.97892428e-01]
