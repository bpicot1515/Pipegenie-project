nohup: ignoring input
Starting Meta-Base Build Process... This will overwrite existing results.

‚ñ∂Ô∏è Processing OpenML Task ID: 232 ---
   Dataset: anneal, Shape: 898.0x39.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
[1/5] ‚öôÔ∏è  Configuration
    - Generations: 200, Population Size: 200
    - Elite Size: 20, Timeout: 3600s
    - Fitness Function: balanced_accuracy_score, Maximization: True
    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_232
    - Random Seed: 42
--- PipeGenie Run Started ---
üïí Start Time: 2025-07-11 12:32:44
[2/5] üå± Initializing evolutionary process...
    - Using meta-learning to warm-start population...
/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/metafeatures.py:42: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.
  features["SkewnessMean"] = float(np.nanmean(skew(X_sample, axis=0)))
/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/metafeatures.py:43: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.
  features["KurtosisMean"] = float(np.nanmean(kurtosis(X_sample, axis=0)))
    - Seeding population with 50 pipeline(s) from past runs.
                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.20957       0.949         0.74696       0.20628       3      4      3.75          0.43301       0.90362       0.949         0.91255       0.010928      3      4      3.85          0.35707   
1      154       0.16615       0.949         0.80912       0.16146       3      4      3.79          0.40731       0.89566       0.949         0.91441       0.012041      3      4      3.75          0.43301   
2      143       0.14615       0.96938       0.85419       0.10927       3      4      3.81          0.3923        0.89245       0.96938       0.92801       0.022262      3      4      3.65          0.47697   
3      133       0.14615       0.97855       0.88605       0.071389      3      4      3.765         0.424         0.9222        0.97855       0.94446       0.012828      3      4      3.75          0.43301   
4      129       0.67923       0.97855       0.90119       0.038143      3      4      3.79          0.40731       0.93226       0.97855       0.94843       0.012645      3      4      3.6           0.4899    
5      114       0.86236       0.97855       0.91872       0.021686      3      4      3.775         0.41758       0.92653       0.97855       0.95423       0.014825      3      4      3.5           0.5       
6      141       0.87427       0.97855       0.92447       0.023439      3      4      3.745         0.43586       0.94426       0.97855       0.96535       0.0091797     3      4      3.5           0.5       
7      136       0.87176       0.97998       0.93235       0.025181      3      4      3.74          0.43863       0.96539       0.97998       0.97108       0.0050346     3      4      3.4           0.4899    
8      147       0.81356       0.98205       0.93988       0.028705      3      4      3.715         0.45141       0.96539       0.98205       0.97487       0.0048707     3      4      3.4           0.4899    
9      127       0.73395       0.98205       0.94689       0.039091      3      4      3.69          0.46249       0.96539       0.98205       0.97546       0.0044704     3      4      3.4           0.4899    
10     124       0.82077       0.98644       0.95421       0.027507      3      4      3.67          0.47021       0.96539       0.98644       0.97776       0.0045609     3      4      3.55          0.49749   
11     146       0.81184       0.98644       0.95653       0.030514      3      4      3.655         0.47537       0.96539       0.98644       0.97826       0.0048815     3      4      3.6           0.4899    
12     115       0.86609       0.98644       0.96336       0.023539      3      4      3.745         0.43586       0.96539       0.98644       0.97823       0.0048723     3      4      3.55          0.49749   
13     110       0.87613       0.98644       0.96868       0.020144      3      4      3.77          0.42083       0.96539       0.98644       0.97889       0.0050291     3      4      3.55          0.49749   
14     122       0.93049       0.98729       0.973         0.013937      3      4      3.85          0.35707       0.96938       0.98729       0.98119       0.0050717     3      4      3.65          0.47697   
15     95        0.89213       0.98729       0.97178       0.019799      3      4      3.89          0.31289       0.96938       0.98729       0.98182       0.005302      3      4      3.65          0.47697   
16     120       0.89793       0.98729       0.97748       0.013057      3      4      3.905         0.29321       0.96938       0.98729       0.98184       0.0053165     3      4      3.65          0.47697   
17     109       0.89389       0.98729       0.97648       0.01732       3      4      3.94          0.23749       0.96938       0.98729       0.98191       0.0053615     3      4      3.65          0.47697   
18     142       0.90721       0.98729       0.97697       0.015864      3      4      3.945         0.22798       0.96938       0.98729       0.982         0.0054419     3      4      3.65          0.47697   
19     105       0.86393       0.98729       0.97797       0.018674      3      4      3.96          0.19596       0.96938       0.98729       0.982         0.0054419     3      4      3.65          0.47697   
20     131       0.91115       0.98729       0.97711       0.016504      3      4      3.965         0.18378       0.96938       0.98729       0.982         0.0054419     3      4      3.65          0.47697   
21     116       0.8957        0.98729       0.97785       0.018332      3      4      3.96          0.19596       0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
22     94        0.85045       0.98729       0.97595       0.026776      3      4      3.955         0.2073        0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
23     86        0.90645       0.98729       0.98279       0.013166      3      4      3.965         0.18378       0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
24     96        0.89968       0.98729       0.98241       0.015464      3      4      3.97          0.17059       0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
25     82        0.92526       0.98729       0.98501       0.009382      3      4      3.975         0.15612       0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
26     101       0.89879       0.98729       0.98381       0.014385      3      4      3.97          0.17059       0.96938       0.98729       0.983         0.0053179     3      4      3.7           0.45826   
27     83        0.91643       0.99029       0.98537       0.0089967     3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
28     98        0.901         0.99029       0.98266       0.015748      3      4      3.98          0.14          0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
29     90        0.92871       0.99029       0.98486       0.009459      3      4      3.97          0.17059       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
30     74        0.89971       0.99029       0.98205       0.016658      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
31     84        0.91029       0.99029       0.9837        0.012842      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
32     85        0.89964       0.99029       0.98162       0.01841       3      4      3.97          0.17059       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
33     82        0.89921       0.99029       0.98261       0.016275      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
34     95        0.89443       0.99029       0.98121       0.018536      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
35     82        0.87879       0.99029       0.98161       0.018375      3      4      3.97          0.17059       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
36     81        0.89836       0.99029       0.98163       0.017812      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
37     79        0.9335        0.99029       0.98461       0.010169      3      4      3.975         0.15612       0.96938       0.99029       0.98315       0.0054767     3      4      3.7           0.45826   
38     84        0.89978       0.99029       0.98144       0.019691      3      4      3.97          0.17059       0.96938       0.99029       0.9833        0.0056269     3      4      3.7           0.45826   
39     94        0.89492       0.99029       0.98358       0.014497      3      4      3.975         0.15612       0.96938       0.99029       0.98395       0.0052866     3      4      3.75          0.43301   
40     83        0.89836       0.99029       0.98298       0.016207      3      4      3.975         0.15612       0.96938       0.99029       0.98395       0.0052866     3      4      3.75          0.43301   
41     70        0.91643       0.99029       0.98555       0.0089945     3      4      3.975         0.15612       0.96938       0.99029       0.98395       0.0052866     3      4      3.75          0.43301   
42     78        0.90076       0.99029       0.98292       0.014949      3      4      3.975         0.15612       0.96938       0.99029       0.98395       0.0052866     3      4      3.75          0.43301   
43     85        0.89836       0.99029       0.97922       0.020261      3      4      3.975         0.15612       0.96938       0.99029       0.9841        0.00542       3      4      3.75          0.43301   
44     102       0.9009        0.99029       0.97611       0.023165      3      4      3.965         0.18378       0.96938       0.99029       0.98451       0.0052934     3      4      3.75          0.43301   
45     102       0.87563       0.99029       0.97556       0.024868      3      4      3.965         0.18378       0.96938       0.99029       0.98453       0.0053033     3      4      3.75          0.43301   
46     101       0.89227       0.99029       0.97833       0.022874      3      4      3.955         0.2073        0.96938       0.99029       0.98453       0.0053033     3      4      3.75          0.43301   
47     88        0.89863       0.99029       0.98076       0.020163      3      4      3.975         0.15612       0.96938       0.99029       0.98453       0.0053033     3      4      3.75          0.43301   
48     87        0.90858       0.99029       0.98324       0.012847      3      4      3.97          0.17059       0.96938       0.99029       0.98453       0.0053033     3      4      3.75          0.43301   
49     88        0.90328       0.99029       0.97982       0.020253      3      4      3.97          0.17059       0.96938       0.99029       0.98469       0.0054227     3      4      3.75          0.43301   
50     93        0.89879       0.99029       0.98103       0.017152      3      4      3.97          0.17059       0.96938       0.99029       0.98488       0.0055485     3      4      3.75          0.43301   
51     99        0.8975        0.99029       0.97883       0.020206      3      4      3.975         0.15612       0.96938       0.99029       0.98511       0.0055233     3      4      3.75          0.43301   
52     99        0.93549       0.99029       0.98307       0.01242       3      4      3.97          0.17059       0.96938       0.99029       0.98514       0.0055324     3      4      3.75          0.43301   
53     89        0.90933       0.99029       0.98256       0.016171      3      4      3.975         0.15612       0.96938       0.99029       0.98529       0.0056284     3      4      3.75          0.43301   
54     101       0.81608       0.99029       0.97789       0.025141      3      4      3.98          0.14          0.96938       0.99029       0.986         0.0056745     3      4      3.8           0.4       
55     91        0.91029       0.99029       0.98033       0.017407      3      4      3.98          0.14          0.96938       0.99029       0.9863        0.0058123     3      4      3.8           0.4       
56     91        0.90636       0.99029       0.97999       0.019552      3      4      3.98          0.14          0.96938       0.99029       0.9863        0.0058123     3      4      3.8           0.4       
57     114       0.87793       0.99029       0.9744        0.024515      3      4      3.98          0.14          0.96938       0.99029       0.98645       0.0058743     3      4      3.8           0.4       
58     88        0.88979       0.99029       0.97372       0.026132      3      4      3.97          0.17059       0.96938       0.99029       0.98664       0.0059336     3      4      3.8           0.4       
59     93        0.90229       0.99029       0.97604       0.02371       3      4      3.98          0.14          0.96938       0.99029       0.98679       0.0059857     3      4      3.8           0.4       
60     116       0.89022       0.99029       0.97171       0.028567      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
61     127       0.89528       0.99029       0.97207       0.02657       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
62     115       0.89443       0.99029       0.9682        0.030697      3      4      3.97          0.17059       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
63     96        0.85715       0.99029       0.97441       0.027313      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
64     92        0.86821       0.99029       0.97928       0.024917      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
65     116       0.87078       0.99029       0.97751       0.027327      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
66     104       0.8895        0.99029       0.98178       0.021721      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
67     103       0.88136       0.99029       0.98446       0.019684      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
68     101       0.82477       0.99029       0.97843       0.029106      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
69     98        0.87914       0.99029       0.98118       0.024         3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
70     103       0.8895        0.99029       0.98409       0.019922      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
71     120       0.87778       0.99029       0.98331       0.023509      3      4      3.97          0.17059       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
72     120       0.81813       0.99029       0.97992       0.029623      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
73     114       0.89921       0.99029       0.98342       0.019616      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
74     111       0.90679       0.99029       0.98795       0.011384      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
75     124       0.81184       0.99029       0.98299       0.024542      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
76     115       0.92685       0.99029       0.9871        0.010917      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
77     123       0.86613       0.99029       0.98287       0.02463       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
78     116       0.83072       0.99029       0.98119       0.028365      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
79     114       0.89843       0.99029       0.98469       0.016955      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
80     121       0.88463       0.99029       0.98459       0.020321      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
81     115       0.89528       0.99029       0.98488       0.018202      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
82     121       0.89399       0.99029       0.98641       0.014636      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
83     101       0.88992       0.99029       0.98523       0.017929      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
84     133       0.89135       0.99029       0.98496       0.018053      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
85     121       0.89221       0.99029       0.98346       0.021814      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
86     126       0.82614       0.99029       0.97936       0.027958      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
87     117       0.90986       0.99029       0.9869        0.013066      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
88     118       0.85398       0.99029       0.98204       0.025869      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
89     117       0.90371       0.99029       0.9851        0.017446      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
90     94        0.90143       0.99029       0.98643       0.014664      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
91     117       0.89922       0.99029       0.98451       0.018701      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
92     111       0.8167        0.99029       0.98041       0.027871      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
93     116       0.88486       0.99029       0.98241       0.02252       3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
94     116       0.89484       0.99029       0.98599       0.014209      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
95     126       0.90143       0.99029       0.98388       0.019747      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
96     118       0.90548       0.99029       0.98538       0.016007      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
97     119       0.90679       0.99029       0.98593       0.015847      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
98     126       0.88356       0.99029       0.9863        0.01513       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
99     110       0.89528       0.99029       0.98331       0.021023      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
100    99        0.87719       0.99029       0.98427       0.02032       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
101    102       0.89528       0.99029       0.98402       0.019676      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
102    100       0.87492       0.99029       0.98146       0.023982      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
103    116       0.75066       0.99029       0.98188       0.028582      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
104    99        0.88771       0.99029       0.98518       0.017861      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
105    108       0.81608       0.99029       0.98088       0.027593      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
106    104       0.79584       0.99029       0.97618       0.035446      3      4      3.97          0.17059       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
107    116       0.90901       0.99029       0.98476       0.017278      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
108    124       0.87793       0.99029       0.98466       0.019088      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
109    115       0.89128       0.99029       0.98179       0.023548      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
110    127       0.89357       0.99029       0.98255       0.021414      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
111    117       0.89485       0.99029       0.985         0.016959      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
112    108       0.90229       0.99029       0.98585       0.015773      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
113    115       0.89022       0.99029       0.98073       0.025021      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
114    126       0.90272       0.99029       0.98246       0.021324      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
115    95        0.90229       0.99029       0.98637       0.015081      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
116    98        0.89386       0.99029       0.98381       0.020546      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
117    128       0.88529       0.99029       0.98406       0.020224      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
118    116       0.90636       0.99029       0.98379       0.019483      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
119    114       0.89443       0.99029       0.98454       0.017833      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
120    123       0.81184       0.99029       0.98146       0.027838      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
121    115       0.88592       0.99029       0.98424       0.02033       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
122    110       0.88668       0.99029       0.98596       0.01691       3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
123    120       0.89036       0.99029       0.98477       0.017006      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
124    111       0.8922        0.99029       0.98265       0.022803      3      4      3.975         0.15612       0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
125    125       0.88413       0.99029       0.98349       0.020706      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
126    121       0.89879       0.99029       0.98307       0.020752      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
127    109       0.89528       0.99029       0.98427       0.018923      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
128    133       0.88321       0.99029       0.98228       0.023429      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
129    127       0.89399       0.99029       0.98167       0.022797      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
130    126       0.8842        0.99029       0.98319       0.021126      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
131    123       0.88936       0.99029       0.98345       0.021085      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
132    113       0.90242       0.99029       0.98661       0.014675      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
133    127       0.91029       0.99029       0.98687       0.012232      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
134    127       0.89528       0.99029       0.98441       0.0186        3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
135    104       0.89921       0.99029       0.98426       0.019465      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
136    104       0.84583       0.99029       0.98751       0.016491      3      4      3.98          0.14          0.96938       0.99029       0.98694       0.0060337     3      4      3.8           0.4       
Timeout of 3600s reached. Stopping evolution.
[3/5] ‚úÖ Evolution finished.
    - Saving run results to meta-knowledge base...
[4/5] üëë Generating and evaluating final ensemble...
    - Ensemble models and weights saved to `ensemble.txt`.
[5/5] ‚ú® PipeGenie run complete.
üïí End Time: 2025-07-11 13:33:00
‚è±Ô∏è Total Elapsed Time: 1:00:15.944911
--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_241
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 13:33:00
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 232.

‚ñ∂Ô∏è Processing OpenML Task ID: 241 ---
   Dataset: balance-scale, Shape: 625.0x5.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.33333       0.95482       0.60652       0.090885      3      4      3.745         0.43586       0.65223       0.91972       0.70784       0.075725      3      4      3.75          0.43301   
INFO:pipegenie_evolution:1      148       0.39231       0.91972       0.62101       0.085829      3      4      3.785         0.41082       0.67553       0.91972       0.77106       0.07383       3      4      3.85          0.35707   
INFO:pipegenie_evolution:2      147       0.40058       0.93743       0.66468       0.082873      3      4      3.8           0.4           0.76729       0.93743       0.8298        0.049303      4      4      4             0         
INFO:pipegenie_evolution:3      149       0.56372       0.95298       0.70189       0.087658      3      4      3.86          0.34699       0.81227       0.95298       0.86751       0.043519      4      4      4             0         
INFO:pipegenie_evolution:4      143       0.59479       0.96371       0.73551       0.094767      3      4      3.925         0.26339       0.85319       0.96371       0.90793       0.034247      3      4      3.95          0.21794   
INFO:pipegenie_evolution:5      122       0.62312       0.96371       0.77423       0.096532      3      4      3.935         0.24653       0.91972       0.96371       0.92914       0.01405       3      4      3.95          0.21794   
INFO:pipegenie_evolution:6      130       0.55939       0.98038       0.8064        0.096899      3      4      3.955         0.2073        0.91972       0.98038       0.94058       0.018815      3      4      3.95          0.21794   
INFO:pipegenie_evolution:7      112       0.5035        0.98038       0.84741       0.10036       3      4      3.955         0.2073        0.93743       0.98038       0.96231       0.011558      3      4      3.95          0.21794   
INFO:pipegenie_evolution:8      120       0.43313       0.98982       0.89027       0.088904      3      4      3.955         0.2073        0.95298       0.98982       0.97331       0.0074659     3      4      3.9           0.3       
INFO:pipegenie_evolution:9      133       0.48123       0.98982       0.91449       0.090456      3      4      3.945         0.22798       0.96371       0.98982       0.97781       0.0051661     3      4      3.9           0.3       
INFO:pipegenie_evolution:10     136       0.33333       0.98982       0.93395       0.093442      3      4      3.95          0.21794       0.97705       0.98982       0.98121       0.0046696     3      4      3.95          0.21794   
INFO:pipegenie_evolution:11     155       0.33333       0.98982       0.93198       0.11371       3      4      3.95          0.21794       0.97816       0.98982       0.98484       0.004818      4      4      4             0         
INFO:pipegenie_evolution:12     136       0.33333       0.98982       0.94645       0.098657      3      4      3.965         0.18378       0.98807       0.98982       0.98914       0.00082422    4      4      4             0         
INFO:pipegenie_evolution:13     146       0.47923       0.98982       0.95998       0.06475       3      4      3.97          0.17059       0.98807       0.98982       0.98956       0.00060265    4      4      4             0         
INFO:pipegenie_evolution:14     146       0.33333       0.98982       0.94211       0.11194       3      4      3.98          0.14          0.98807       0.98982       0.98964       0.0005099     4      4      4             0         
INFO:pipegenie_evolution:15     139       0.47625       0.99325       0.95562       0.084389      3      4      3.995         0.070534      0.98807       0.99325       0.98998       0.0010646     4      4      4             0         
INFO:pipegenie_evolution:16     144       0.79049       0.99325       0.97454       0.027849      3      4      3.99          0.099499      0.98974       0.99325       0.99098       0.0011046     4      4      4             0         
INFO:pipegenie_evolution:17     145       0.73863       0.99325       0.97447       0.03411       3      4      3.99          0.099499      0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
INFO:pipegenie_evolution:18     152       0.78806       0.99325       0.97914       0.0244        3      4      3.995         0.070534      0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
INFO:pipegenie_evolution:19     135       0.66253       0.99325       0.98207       0.035561      3      4      3.99          0.099499      0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
INFO:pipegenie_evolution:20     139       0.61187       0.99325       0.97582       0.055404      3      4      3.995         0.070534      0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
INFO:pipegenie_evolution:21     120       0.74806       0.99325       0.98418       0.031425      4      4      4             0             0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
INFO:pipegenie_evolution:22     140       0.64988       0.99325       0.98289       0.041275      4      4      4             0             0.98982       0.99325       0.99158       0.00066196    4      4      4             0         
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 14:35:02
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:02:01.856469
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_245
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 14:35:03
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 241.

‚ñ∂Ô∏è Processing OpenML Task ID: 245 ---
   Dataset: breast-w, Shape: 699.0x10.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.6486        0.98573       0.95625       0.030597      3      4      3.74          0.43863       0.97228       0.98573       0.97736       0.0039991     3      4      3.75          0.43301   
INFO:pipegenie_evolution:1      139       0.50678       0.98573       0.95762       0.045042      3      4      3.76          0.42708       0.97561       0.98573       0.97965       0.0030985     3      4      3.9           0.3       
INFO:pipegenie_evolution:2      147       0.91392       0.98729       0.96594       0.011796      3      4      3.78          0.41425       0.97895       0.98729       0.98219       0.0026688     3      4      3.95          0.21794   
INFO:pipegenie_evolution:3      145       0.95226       0.98729       0.9713        0.0082383     3      4      3.81          0.3923        0.98072       0.98729       0.98429       0.0016395     3      4      3.9           0.3       
INFO:pipegenie_evolution:4      150       0.95739       0.98729       0.97507       0.007437      3      4      3.825         0.37997       0.98384       0.98729       0.98523       0.0014799     3      4      3.95          0.21794   
INFO:pipegenie_evolution:5      129       0.95061       0.98729       0.97662       0.0087341     3      4      3.905         0.29321       0.98384       0.98729       0.98591       0.0014068     4      4      4             0         
INFO:pipegenie_evolution:6      150       0.95237       0.98729       0.97863       0.0073017     3      4      3.94          0.23749       0.98573       0.98729       0.98682       0.00071603    4      4      4             0         
INFO:pipegenie_evolution:7      156       0.96302       0.98729       0.98202       0.004727      3      4      3.93          0.25515       0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:8      142       0.95228       0.98729       0.98167       0.0067425     3      4      3.94          0.23749       0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:9      155       0.96706       0.98729       0.98307       0.0047948     3      4      3.93          0.25515       0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:10     133       0.97051       0.98729       0.98445       0.0035355     3      4      3.955         0.2073        0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:11     145       0.97051       0.98729       0.9845        0.003511      3      4      3.98          0.14          0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:12     145       0.96184       0.98729       0.98368       0.0051806     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:13     149       0.96582       0.98729       0.98404       0.0045836     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:14     130       0.96249       0.98729       0.98412       0.0045653     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:15     142       0.95603       0.98729       0.98429       0.0053041     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:16     156       0.96916       0.98729       0.9849        0.0039802     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:17     140       0.97384       0.98729       0.98554       0.0031742     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:18     151       0.97396       0.98729       0.98591       0.0026648     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:19     146       0.97373       0.98729       0.98553       0.0034226     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:20     124       0.97384       0.98729       0.9858        0.0032011     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:21     125       0.97706       0.98729       0.98604       0.0026291     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:22     133       0.96582       0.98729       0.98569       0.0035945     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:23     133       0.96916       0.98729       0.98568       0.0036642     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:24     136       0.97384       0.98729       0.98604       0.0026695     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:25     145       0.97573       0.98729       0.98587       0.0027564     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:26     124       0.97718       0.98729       0.98639       0.0021981     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:27     133       0.9755        0.98729       0.98609       0.0026131     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:28     149       0.97384       0.98729       0.98591       0.0027067     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:29     140       0.97384       0.98729       0.98603       0.0027909     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:30     151       0.97228       0.98729       0.98553       0.0034638     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:31     145       0.97718       0.98729       0.98596       0.0025208     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:32     143       0.9755        0.98729       0.98613       0.0024504     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:33     124       0.97217       0.98729       0.98603       0.0029134     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:34     141       0.96904       0.98729       0.986         0.0035418     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:35     154       0.96706       0.98729       0.98576       0.003841      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:36     122       0.97051       0.98729       0.98577       0.0035023     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:37     145       0.96916       0.98729       0.98531       0.004145      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:38     125       0.97384       0.98729       0.98594       0.0031617     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:39     133       0.9755        0.98729       0.98638       0.0021629     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:40     137       0.97051       0.98729       0.98601       0.0031304     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:41     130       0.96582       0.98729       0.98541       0.0039095     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:42     131       0.97228       0.98729       0.98593       0.0031195     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:43     139       0.97249       0.98729       0.9857        0.0033734     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:44     117       0.96184       0.98729       0.98529       0.0049015     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:45     139       0.96883       0.98729       0.98537       0.003773      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:46     132       0.97217       0.98729       0.98585       0.0033253     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:47     143       0.96561       0.98729       0.98551       0.0038833     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:48     152       0.97384       0.98729       0.98564       0.0034461     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:49     144       0.97394       0.98729       0.98612       0.0025164     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:50     144       0.97373       0.98729       0.98597       0.0029761     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:51     145       0.97051       0.98729       0.98552       0.0034858     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:52     145       0.9704        0.98729       0.98571       0.0031892     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:53     149       0.96582       0.98729       0.98538       0.0038095     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:54     140       0.97561       0.98729       0.98583       0.0028133     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:55     138       0.97373       0.98729       0.98568       0.003337      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:56     120       0.97718       0.98729       0.98631       0.0022382     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:57     128       0.97561       0.98729       0.9863        0.0024344     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:58     129       0.97051       0.98729       0.98594       0.003383      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:59     136       0.97051       0.98729       0.98585       0.0034838     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:60     130       0.97228       0.98729       0.98589       0.0031021     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:61     131       0.97384       0.98729       0.9861        0.0026229     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:62     149       0.97405       0.98729       0.98596       0.0025938     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:63     148       0.97384       0.98729       0.98551       0.0035051     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:64     142       0.97051       0.98729       0.98576       0.0034186     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:65     146       0.9727        0.98729       0.98609       0.0026489     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:66     141       0.97051       0.98729       0.98597       0.0031612     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:67     123       0.97718       0.98729       0.98651       0.0020094     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:68     133       0.9678        0.98729       0.98573       0.0036015     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:69     130       0.97582       0.98729       0.98609       0.0024367     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:70     139       0.97373       0.98729       0.98593       0.0030103     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:71     145       0.97384       0.98729       0.98556       0.0031426     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:72     138       0.97384       0.98729       0.98576       0.0030511     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:73     129       0.96426       0.98729       0.98547       0.0041944     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:74     126       0.97384       0.98729       0.98596       0.0028423     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:75     120       0.97384       0.98729       0.98585       0.0029153     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:76     135       0.97228       0.98729       0.98537       0.0036314     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:77     116       0.9726        0.98729       0.98575       0.0030844     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:78     122       0.97051       0.98729       0.98598       0.0031061     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:79     138       0.97228       0.98729       0.98586       0.0034306     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:80     129       0.9704        0.98729       0.98546       0.0036107     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:81     128       0.97384       0.98729       0.98591       0.0028348     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:82     129       0.97384       0.98729       0.9858        0.002826      4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:83     130       0.97384       0.98729       0.98583       0.0030027     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:84     133       0.9724        0.98729       0.98619       0.0026868     3      4      3.995         0.070534      0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:85     130       0.97594       0.98729       0.98594       0.0025783     4      4      4             0             0.98573       0.98729       0.98721       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:86     124       0.97384       0.98729       0.98601       0.0027447     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:87     128       0.97906       0.98729       0.98656       0.0017559     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:88     145       0.97384       0.98729       0.98577       0.0030993     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:89     129       0.97384       0.98729       0.98615       0.0026803     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:90     122       0.96771       0.98729       0.98602       0.0033345     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:91     123       0.97384       0.98729       0.986         0.0028522     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:92     105       0.96184       0.98729       0.98586       0.0045542     3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:93     108       0.97249       0.98729       0.98632       0.0026579     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:94     132       0.96916       0.98729       0.98582       0.00362       4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:95     136       0.96916       0.98729       0.98548       0.0036274     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:96     131       0.96916       0.98729       0.98603       0.0028378     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:97     126       0.97051       0.98729       0.98585       0.0031607     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:98     132       0.97561       0.98729       0.98595       0.0026616     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:99     142       0.96883       0.98729       0.9856        0.0037985     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:100    119       0.97718       0.98729       0.98648       0.0019634     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:101    106       0.97895       0.98729       0.98649       0.001894      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:102    132       0.97228       0.98729       0.98568       0.0035203     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:103    119       0.97384       0.98729       0.98559       0.0030798     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:104    125       0.97072       0.98729       0.98556       0.0033867     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:105    119       0.97051       0.98729       0.98604       0.0029412     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:106    121       0.97104       0.98729       0.98583       0.0031744     3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:107    138       0.96718       0.98729       0.98549       0.0034719     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:108    129       0.96916       0.98729       0.98568       0.003306      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:109    123       0.96739       0.98729       0.98593       0.0032106     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:110    123       0.96906       0.98729       0.9855        0.0035854     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:111    131       0.97384       0.98729       0.98578       0.0028553     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:112    118       0.96561       0.98729       0.98576       0.0034226     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:113    113       0.97582       0.98729       0.9863        0.0021812     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:114    124       0.97051       0.98729       0.98543       0.0038298     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:115    138       0.97384       0.98729       0.98559       0.0030248     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:116    115       0.96184       0.98729       0.9854        0.0041778     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:117    122       0.97217       0.98729       0.98544       0.0033903     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:118    120       0.97228       0.98729       0.98586       0.0027525     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:119    126       0.96373       0.98729       0.98591       0.0033565     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:120    124       0.97729       0.98729       0.98637       0.001979      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:121    115       0.97384       0.98729       0.98622       0.0024484     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:122    122       0.97561       0.98729       0.98625       0.0022516     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:123    119       0.97405       0.98729       0.98617       0.0025386     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:124    107       0.97384       0.98729       0.98632       0.0024443     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:125    126       0.97718       0.98729       0.98613       0.002372      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:126    126       0.96916       0.98729       0.9859        0.0032166     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:127    137       0.97417       0.98729       0.98585       0.0029395     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:128    119       0.96184       0.98729       0.98557       0.0040406     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:129    112       0.97249       0.98729       0.98596       0.0030544     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:130    117       0.97718       0.98729       0.98642       0.0021039     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:131    115       0.97906       0.98729       0.98663       0.0017107     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:132    127       0.97384       0.98729       0.98615       0.0026282     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:133    129       0.96916       0.98729       0.98525       0.004067      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:134    113       0.97706       0.98729       0.98611       0.0023725     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:135    130       0.97373       0.98729       0.9859        0.0031076     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:136    139       0.97384       0.98729       0.98601       0.0027412     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:137    133       0.96184       0.98729       0.98528       0.0045995     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:138    127       0.97072       0.98729       0.98575       0.003541      3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:139    114       0.96739       0.98729       0.98545       0.004014      3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:140    137       0.97373       0.98729       0.98612       0.0027632     3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:141    138       0.96739       0.98729       0.98574       0.0036538     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:142    117       0.97582       0.98729       0.98634       0.0021908     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:143    111       0.97384       0.98729       0.9863        0.0027374     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:144    113       0.97072       0.98729       0.98615       0.0028259     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:145    124       0.96582       0.98729       0.98548       0.0038392     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:146    127       0.97384       0.98729       0.98637       0.0025658     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:147    128       0.97228       0.98729       0.98588       0.0034635     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:148    124       0.9755        0.98729       0.98656       0.0021159     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:149    128       0.96739       0.98729       0.9854        0.004153      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:150    127       0.96426       0.98729       0.98535       0.0043338     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:151    119       0.96916       0.98729       0.98589       0.0032791     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:152    129       0.97249       0.98729       0.986         0.0029052     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:153    136       0.97373       0.98729       0.98594       0.0027772     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:154    130       0.97072       0.98729       0.98625       0.002625      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:155    133       0.97051       0.98729       0.98593       0.003095      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:156    115       0.96883       0.98729       0.98584       0.0034466     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:157    130       0.97093       0.98729       0.98618       0.0028425     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:158    119       0.97384       0.98729       0.98618       0.0025724     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:159    121       0.96916       0.98729       0.98581       0.0038877     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:160    117       0.97384       0.98729       0.98639       0.0025674     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:161    139       0.97561       0.98729       0.98621       0.002441      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:162    122       0.97384       0.98729       0.98592       0.0030886     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:163    115       0.97417       0.98729       0.98633       0.0025192     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:164    131       0.97228       0.98729       0.98606       0.0029144     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:165    125       0.96184       0.98729       0.98561       0.0041181     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:166    115       0.96792       0.98729       0.98569       0.0039207     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:167    120       0.97417       0.98729       0.98585       0.0027999     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:168    129       0.97895       0.98729       0.98639       0.0019261     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:169    129       0.97718       0.98729       0.9865        0.0021335     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:170    127       0.98051       0.98729       0.98658       0.0017942     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:171    126       0.96184       0.98729       0.98535       0.0046074     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:172    109       0.96228       0.98729       0.98553       0.0044714     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:173    123       0.96883       0.98729       0.98541       0.0037623     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:174    102       0.97249       0.98729       0.98623       0.0026299     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:175    114       0.96582       0.98729       0.98554       0.004338      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:176    123       0.96302       0.98729       0.98591       0.0037302     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:177    114       0.97051       0.98729       0.98626       0.0027962     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:178    98        0.96184       0.98729       0.98582       0.0042011     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:179    129       0.97217       0.98729       0.98645       0.0023699     3      4      3.995         0.070534      0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:180    123       0.97384       0.98729       0.98649       0.0021536     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:181    126       0.96718       0.98729       0.98553       0.004079      4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:182    114       0.96883       0.98729       0.98618       0.0030426     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:183    124       0.97718       0.98729       0.98639       0.0021124     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:184    119       0.97561       0.98729       0.9864        0.0021751     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:185    132       0.97417       0.98729       0.98663       0.0020396     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:186    115       0.96916       0.98729       0.98612       0.0029659     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:187    114       0.97072       0.98729       0.98559       0.0036586     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:188    130       0.96792       0.98729       0.98585       0.0036201     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:189    108       0.97883       0.98729       0.98662       0.0017506     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:190    124       0.97718       0.98729       0.98653       0.0021177     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:191    114       0.97561       0.98729       0.9866        0.0020358     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:192    113       0.97405       0.98729       0.98643       0.0023271     4      4      4             0             0.98729       0.98729       0.98729       2.2204e-16    4      4      4             0         
INFO:pipegenie_evolution:193    133       0.96729       0.98885       0.98563       0.0038959     4      4      4             0             0.98729       0.98885       0.98737       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:194    134       0.96883       0.98885       0.98605       0.0033651     4      4      4             0             0.98729       0.98885       0.98737       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:195    124       0.97072       0.98885       0.9861        0.0030713     4      4      4             0             0.98729       0.98885       0.98737       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:196    135       0.97384       0.98885       0.98599       0.0028162     4      4      4             0             0.98729       0.98885       0.98737       0.00034054    4      4      4             0         
INFO:pipegenie_evolution:197    123       0.97237       0.98885       0.98579       0.0031811     4      4      4             0             0.98729       0.98885       0.98745       0.00046875    4      4      4             0         
INFO:pipegenie_evolution:198    121       0.94541       0.98885       0.98589       0.0049904     4      4      4             0             0.98729       0.98885       0.98745       0.00046875    4      4      4             0         
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 15:35:18
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:15.300958
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_273
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 15:35:18
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 245.

‚ñ∂Ô∏è Processing OpenML Task ID: 273 ---
   Dataset: spambase, Shape: 4601.0x58.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.5           0.95094       0.88951       0.079394      3      4      3.735         0.44133       0.94171       0.94891       0.94566       0.002158      3      4      3.85          0.35707   
INFO:pipegenie_evolution:1      144       0.5           0.94891       0.90655       0.06226       3      4      3.8           0.4           0.94471       0.94891       0.9474        0.0011241     3      4      3.9           0.3       
INFO:pipegenie_evolution:2      144       0.77501       0.95253       0.92679       0.026909      3      4      3.81          0.3923        0.94725       0.95253       0.94882       0.0012789     3      4      3.7           0.45826   
INFO:pipegenie_evolution:3      141       0.8945        0.95253       0.93911       0.011633      3      4      3.85          0.35707       0.94822       0.95253       0.94997       0.0012024     3      4      3.55          0.49749   
INFO:pipegenie_evolution:4      132       0.92435       0.95268       0.94477       0.0050196     3      4      3.81          0.3923        0.94841       0.95268       0.95091       0.0010047     3      4      3.7           0.45826   
INFO:pipegenie_evolution:5      132       0.93667       0.95283       0.94692       0.0035037     3      4      3.785         0.41082       0.94841       0.95283       0.95149       0.0010285     3      4      3.7           0.45826   
INFO:pipegenie_evolution:6      163       0.93748       0.95353       0.94813       0.0027139     3      4      3.75          0.43301       0.95104       0.95353       0.95204       0.00065401    3      4      3.8           0.4       
INFO:pipegenie_evolution:7      135       0.93897       0.95353       0.94893       0.0024803     3      4      3.75          0.43301       0.95137       0.95353       0.95233       0.00055478    3      4      3.75          0.43301   
INFO:pipegenie_evolution:8      128       0.93974       0.95514       0.94954       0.0026981     3      4      3.72          0.449         0.95137       0.95514       0.95291       0.00099844    3      4      3.55          0.49749   
INFO:pipegenie_evolution:9      142       0.94224       0.95514       0.94981       0.0025293     3      4      3.74          0.43863       0.95152       0.95514       0.95331       0.00089951    3      4      3.55          0.49749   
INFO:pipegenie_evolution:10     132       0.94304       0.95514       0.95037       0.002376      3      4      3.725         0.44651       0.95213       0.95514       0.95352       0.00079101    3      4      3.55          0.49749   
INFO:pipegenie_evolution:11     150       0.93982       0.95514       0.95009       0.0027632     3      4      3.655         0.47537       0.95253       0.95514       0.95386       0.00075848    3      4      3.6           0.4899    
INFO:pipegenie_evolution:12     136       0.94361       0.95514       0.951         0.002496      3      4      3.58          0.49356       0.95268       0.95514       0.95432       0.00063306    3      4      3.6           0.4899    
INFO:pipegenie_evolution:13     136       0.93694       0.95514       0.95088       0.0030713     3      4      3.53          0.4991        0.95379       0.95514       0.95447       0.00044505    3      4      3.6           0.4899    
INFO:pipegenie_evolution:14     132       0.94329       0.95514       0.95129       0.0026131     3      4      3.49          0.4999        0.95379       0.95514       0.95449       0.00042207    3      4      3.6           0.4899    
INFO:pipegenie_evolution:15     121       0.94194       0.95639       0.95133       0.0028051     3      4      3.48          0.4996        0.95379       0.95639       0.95464       0.00056111    3      4      3.55          0.49749   
INFO:pipegenie_evolution:16     134       0.94582       0.95639       0.95179       0.002421      3      4      3.515         0.49977       0.95312       0.95639       0.95501       0.00088927    3      4      3.65          0.47697   
INFO:pipegenie_evolution:17     136       0.92981       0.95639       0.95142       0.0040607     3      4      3.435         0.49576       0.95312       0.95639       0.95524       0.00084396    3      4      3.55          0.49749   
INFO:pipegenie_evolution:18     135       0.94605       0.95677       0.95238       0.0024951     3      4      3.435         0.49576       0.95312       0.95677       0.95548       0.00084268    3      4      3.55          0.49749   
INFO:pipegenie_evolution:19     129       0.88877       0.95677       0.95126       0.0075802     3      4      3.48          0.4996        0.95436       0.95677       0.95558       0.00065831    3      4      3.5           0.5       
INFO:pipegenie_evolution:20     133       0.94617       0.95772       0.95268       0.0025933     3      4      3.545         0.49797       0.95436       0.95772       0.95593       0.00080826    3      4      3.7           0.45826   
INFO:pipegenie_evolution:21     123       0.93555       0.95772       0.9526        0.0033595     3      4      3.56          0.49639       0.95436       0.95772       0.95602       0.00079843    3      4      3.7           0.45826   
INFO:pipegenie_evolution:22     137       0.92534       0.95772       0.95275       0.0039317     3      4      3.655         0.47537       0.95436       0.95772       0.95624       0.00078613    3      4      3.7           0.45826   
INFO:pipegenie_evolution:23     129       0.91305       0.95772       0.95234       0.0048381     3      4      3.655         0.47537       0.95503       0.95772       0.95644       0.00065077    3      4      3.55          0.49749   
INFO:pipegenie_evolution:24     124       0.94316       0.95772       0.9533        0.0028035     3      4      3.605         0.48885       0.95503       0.95772       0.9564        0.00065489    3      4      3.65          0.47697   
INFO:pipegenie_evolution:25     132       0.93759       0.95825       0.95278       0.0037505     3      4      3.585         0.49272       0.9557        0.95825       0.95661       0.00062109    3      4      3.6           0.4899    
INFO:pipegenie_evolution:26     110       0.94279       0.95825       0.9535        0.0030696     3      4      3.555         0.49697       0.9557        0.95825       0.95661       0.00062109    3      4      3.6           0.4899    
INFO:pipegenie_evolution:27     122       0.91502       0.95825       0.95304       0.0055039     3      4      3.575         0.49434       0.9557        0.95825       0.95671       0.00070482    3      4      3.55          0.49749   
INFO:pipegenie_evolution:28     120       0.93494       0.95825       0.95356       0.0034531     3      4      3.52          0.4996        0.9557        0.95825       0.9568        0.00075906    3      4      3.5           0.5       
INFO:pipegenie_evolution:29     136       0.93074       0.95825       0.95306       0.0043547     3      4      3.505         0.49997       0.9557        0.95825       0.95685       0.00073345    3      4      3.5           0.5       
INFO:pipegenie_evolution:30     117       0.93365       0.95825       0.95369       0.0039597     3      4      3.52          0.4996        0.9557        0.95825       0.95696       0.00078225    3      4      3.5           0.5       
INFO:pipegenie_evolution:31     119       0.9461        0.95866       0.95396       0.0030922     3      4      3.55          0.49749       0.9557        0.95866       0.95714       0.00089148    3      4      3.5           0.5       
INFO:pipegenie_evolution:32     118       0.94496       0.95866       0.95386       0.0032029     3      4      3.5           0.5           0.9557        0.95866       0.95735       0.00091916    3      4      3.5           0.5       
INFO:pipegenie_evolution:33     107       0.94011       0.95866       0.95439       0.0033132     3      4      3.47          0.4991        0.95636       0.95866       0.9577        0.00083937    3      4      3.45          0.49749   
INFO:pipegenie_evolution:34     123       0.94211       0.95866       0.9544        0.0032856     3      4      3.415         0.49272       0.95639       0.95866       0.95783       0.00071525    3      4      3.35          0.47697   
INFO:pipegenie_evolution:35     127       0.94187       0.95866       0.95478       0.0029796     3      4      3.455         0.49797       0.95639       0.95866       0.95783       0.00071525    3      4      3.35          0.47697   
INFO:pipegenie_evolution:36     109       0.93167       0.95866       0.95491       0.0044203     3      4      3.425         0.49434       0.95639       0.95866       0.95795       0.00066691    3      4      3.45          0.49749   
INFO:pipegenie_evolution:37     111       0.92007       0.95866       0.95527       0.0047279     3      4      3.38          0.48539       0.95639       0.95866       0.958         0.00064359    3      4      3.35          0.47697   
INFO:pipegenie_evolution:38     111       0.94629       0.95866       0.9559        0.0028505     3      4      3.32          0.46648       0.95639       0.95866       0.95787       0.00070017    3      4      3.35          0.47697   
INFO:pipegenie_evolution:39     118       0.94546       0.95866       0.95639       0.0028548     3      4      3.295         0.45604       0.95639       0.95866       0.95799       0.00067034    3      4      3.45          0.49749   
INFO:pipegenie_evolution:40     103       0.94074       0.95866       0.95668       0.0033148     3      4      3.295         0.45604       0.95639       0.95866       0.95789       0.00070112    3      4      3.4           0.4899    
INFO:pipegenie_evolution:41     122       0.94582       0.95866       0.95699       0.0027869     3      4      3.345         0.47537       0.95639       0.95866       0.95799       0.00067034    3      4      3.4           0.4899    
INFO:pipegenie_evolution:42     93        0.94335       0.95866       0.95757       0.0025415     3      4      3.405         0.49089       0.95639       0.95866       0.95799       0.00067034    3      4      3.4           0.4899    
INFO:pipegenie_evolution:43     102       0.93919       0.95866       0.95721       0.0032637     3      4      3.43          0.49508       0.95639       0.95866       0.95799       0.00067034    3      4      3.4           0.4899    
INFO:pipegenie_evolution:44     102       0.94111       0.95866       0.9575        0.0027724     3      4      3.45          0.49749       0.95639       0.95866       0.95799       0.00067034    3      4      3.4           0.4899    
INFO:pipegenie_evolution:45     81        0.94388       0.95921       0.95793       0.0023719     3      4      3.525         0.49937       0.95639       0.95921       0.95801       0.00070752    3      4      3.45          0.49749   
INFO:pipegenie_evolution:46     76        0.94565       0.95921       0.95801       0.0022385     3      4      3.585         0.49272       0.95639       0.95921       0.95819       0.0006531     3      4      3.45          0.49749   
INFO:pipegenie_evolution:47     72        0.93302       0.95921       0.95749       0.0042519     3      4      3.58          0.49356       0.95639       0.95921       0.95819       0.0006531     3      4      3.45          0.49749   
INFO:pipegenie_evolution:48     66        0.94583       0.95921       0.95817       0.0020662     3      4      3.57          0.49508       0.95639       0.95921       0.9582        0.00067098    3      4      3.4           0.4899    
INFO:pipegenie_evolution:49     80        0.94361       0.95921       0.9581        0.0022118     3      4      3.6           0.4899        0.95639       0.95921       0.95826       0.00070517    3      4      3.4           0.4899    
INFO:pipegenie_evolution:50     97        0.94291       0.95921       0.95772       0.0030462     3      4      3.64          0.48          0.95639       0.95921       0.95826       0.00070517    3      4      3.4           0.4899    
INFO:pipegenie_evolution:51     76        0.9459        0.95974       0.95829       0.0019674     3      4      3.61          0.48775       0.95639       0.95974       0.95828       0.00074952    3      4      3.4           0.4899    
INFO:pipegenie_evolution:52     106       0.94414       0.95974       0.95812       0.0024918     3      4      3.605         0.48885       0.95639       0.95974       0.95834       0.00081134    3      4      3.4           0.4899    
INFO:pipegenie_evolution:53     113       0.94902       0.95974       0.95843       0.0016454     3      4      3.595         0.49089       0.95639       0.95974       0.95837       0.00084761    3      4      3.45          0.49749   
INFO:pipegenie_evolution:54     102       0.9466        0.95974       0.95843       0.0020395     3      4      3.62          0.48539       0.95639       0.95974       0.95841       0.00086649    3      4      3.45          0.49749   
INFO:pipegenie_evolution:55     110       0.94011       0.95974       0.95811       0.0030259     3      4      3.52          0.4996        0.95639       0.95974       0.95843       0.00088056    3      4      3.5           0.5       
INFO:pipegenie_evolution:56     92        0.94714       0.95974       0.95861       0.0019893     3      4      3.485         0.49977       0.9568        0.95974       0.95859       0.00079133    3      4      3.45          0.49749   
INFO:pipegenie_evolution:57     89        0.94238       0.95988       0.9583        0.0031653     3      4      3.56          0.49639       0.9568        0.95988       0.9586        0.00080195    3      4      3.45          0.49749   
INFO:pipegenie_evolution:58     100       0.94128       0.95988       0.95835       0.0033121     3      4      3.59          0.49183       0.9568        0.95988       0.9587        0.00081139    3      4      3.45          0.49749   
INFO:pipegenie_evolution:59     111       0.94305       0.95988       0.95847       0.002951      3      4      3.57          0.49508       0.9568        0.95988       0.9587        0.00081139    3      4      3.45          0.49749   
INFO:pipegenie_evolution:60     109       0.94508       0.95988       0.95884       0.0024839     3      4      3.58          0.49356       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:61     101       0.94265       0.95988       0.95876       0.002637      3      4      3.635         0.48143       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:62     91        0.94468       0.95988       0.95873       0.0026564     3      4      3.67          0.47021       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:63     90        0.94919       0.95988       0.95909       0.002003      3      4      3.69          0.46249       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:64     96        0.93992       0.95988       0.95853       0.0037477     3      4      3.63          0.4828        0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:65     104       0.94282       0.95988       0.95865       0.0031704     3      4      3.66          0.47371       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:66     96        0.94485       0.95988       0.95886       0.0028138     3      4      3.61          0.48775       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:67     75        0.95105       0.95988       0.95951       0.001247      3      4      3.575         0.49434       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:68     67        0.94522       0.95988       0.95926       0.0021474     3      4      3.565         0.49576       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:69     63        0.94958       0.95988       0.9595        0.001534      3      4      3.59          0.49183       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:70     67        0.94554       0.95988       0.95906       0.0027844     3      4      3.61          0.48775       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:71     59        0.93951       0.95988       0.95894       0.0032665     3      4      3.6           0.4899        0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:72     65        0.94945       0.95988       0.95923       0.0020648     3      4      3.605         0.48885       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:73     65        0.9493        0.95988       0.95933       0.0019105     3      4      3.68          0.46648       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:74     86        0.94607       0.95988       0.95884       0.0030226     3      4      3.665         0.47199       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:75     75        0.94671       0.95988       0.95928       0.0022089     3      4      3.63          0.4828        0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:76     77        0.94619       0.95988       0.95951       0.0015043     3      4      3.585         0.49272       0.9568        0.95988       0.95871       0.00082083    3      4      3.5           0.5       
INFO:pipegenie_evolution:77     72        0.94576       0.96002       0.95945       0.0018018     3      4      3.59          0.49183       0.9568        0.96002       0.95872       0.00083114    3      4      3.45          0.49749   
INFO:pipegenie_evolution:78     65        0.94926       0.96002       0.95944       0.0016986     3      4      3.565         0.49576       0.9568        0.96002       0.95877       0.00086914    3      4      3.5           0.5       
INFO:pipegenie_evolution:79     79        0.94114       0.96002       0.95853       0.0039412     3      4      3.595         0.49089       0.9568        0.96002       0.95877       0.00086914    3      4      3.5           0.5       
INFO:pipegenie_evolution:80     74        0.94862       0.96002       0.95933       0.0019365     3      4      3.61          0.48775       0.9568        0.96002       0.95878       0.00087841    3      4      3.5           0.5       
INFO:pipegenie_evolution:81     96        0.83467       0.96002       0.95592       0.016484      3      4      3.585         0.49272       0.9568        0.96002       0.95878       0.00087841    3      4      3.5           0.5       
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 16:36:04
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:46.271770
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_275
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 16:36:05
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 273.

‚ñ∂Ô∏è Processing OpenML Task ID: 275 ---
   Dataset: splice, Shape: 3190.0x61.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.3359        0.96416       0.8895        0.13342       3      4      3.74          0.43863       0.95456       0.96416       0.95772       0.0023105     3      4      3.9           0.3       
INFO:pipegenie_evolution:1      150       0.33333       0.96416       0.90236       0.10374       3      4      3.755         0.43009       0.95591       0.96416       0.95919       0.0021406     3      4      3.7           0.45826   
INFO:pipegenie_evolution:2      141       0.33333       0.96416       0.92887       0.080319      3      4      3.775         0.41758       0.95646       0.96416       0.96022       0.0017375     3      4      3.8           0.4       
INFO:pipegenie_evolution:3      151       0.75577       0.96416       0.94627       0.030896      3      4      3.78          0.41425       0.95646       0.96416       0.96094       0.0018196     3      4      3.8           0.4       
INFO:pipegenie_evolution:4      147       0.74649       0.96416       0.95306       0.020286      3      4      3.78          0.41425       0.9604        0.96416       0.96197       0.0012295     3      4      3.8           0.4       
INFO:pipegenie_evolution:5      136       0.76517       0.96416       0.95462       0.020125      3      4      3.735         0.44133       0.96066       0.96416       0.96252       0.0012151     3      4      3.85          0.35707   
INFO:pipegenie_evolution:6      141       0.90791       0.96416       0.95675       0.0072508     3      4      3.79          0.40731       0.96066       0.96416       0.96264       0.0011719     3      4      3.85          0.35707   
INFO:pipegenie_evolution:7      127       0.94197       0.96416       0.95828       0.0043196     3      4      3.84          0.36661       0.96066       0.96416       0.96266       0.0011678     3      4      3.9           0.3       
INFO:pipegenie_evolution:8      152       0.94343       0.96484       0.9591        0.0037734     3      4      3.845         0.3619        0.96005       0.96484       0.96284       0.0013316     3      4      3.85          0.35707   
INFO:pipegenie_evolution:9      117       0.94916       0.96484       0.95972       0.0033876     3      4      3.86          0.34699       0.96066       0.96484       0.96312       0.0013175     3      4      3.85          0.35707   
INFO:pipegenie_evolution:10     124       0.93804       0.96484       0.95984       0.0043004     3      4      3.885         0.31902       0.96066       0.96484       0.9635        0.0011094     3      4      3.95          0.21794   
INFO:pipegenie_evolution:11     132       0.93533       0.96589       0.96042       0.004243      3      4      3.905         0.29321       0.96358       0.96589       0.96435       0.00060489    3      4      3.95          0.21794   
INFO:pipegenie_evolution:12     121       0.93981       0.96589       0.96037       0.0041658     3      4      3.9           0.3           0.96326       0.96589       0.96446       0.00071621    3      4      3.95          0.21794   
INFO:pipegenie_evolution:13     142       0.93708       0.96639       0.96021       0.0051925     3      4      3.89          0.31289       0.96358       0.96639       0.96488       0.00066742    4      4      4             0         
INFO:pipegenie_evolution:14     132       0.93533       0.96639       0.96056       0.0051938     3      4      3.925         0.26339       0.96358       0.96639       0.96488       0.00069747    4      4      4             0         
INFO:pipegenie_evolution:15     129       0.92944       0.96639       0.96074       0.005418      3      4      3.9           0.3           0.96358       0.96639       0.96501       0.00067049    4      4      4             0         
INFO:pipegenie_evolution:16     129       0.931         0.96639       0.96111       0.0049396     3      4      3.905         0.29321       0.96358       0.96639       0.96512       0.00059255    4      4      4             0         
INFO:pipegenie_evolution:17     115       0.95146       0.96639       0.96206       0.0030352     3      4      3.92          0.27129       0.96358       0.96639       0.96516       0.00060733    4      4      4             0         
INFO:pipegenie_evolution:18     138       0.93966       0.96639       0.96206       0.0038158     3      4      3.965         0.18378       0.96358       0.96639       0.96529       0.00062812    4      4      4             0         
INFO:pipegenie_evolution:19     143       0.95136       0.96755       0.96228       0.003083      3      4      3.98          0.14          0.96358       0.96755       0.96546       0.00082463    4      4      4             0         
INFO:pipegenie_evolution:20     102       0.95336       0.96755       0.96275       0.0029611     4      4      4             0             0.96358       0.96755       0.96556       0.00084738    4      4      4             0         
INFO:pipegenie_evolution:21     112       0.95095       0.96755       0.96299       0.0030449     4      4      4             0             0.96484       0.96755       0.96582       0.00080059    4      4      4             0         
INFO:pipegenie_evolution:22     124       0.95242       0.96758       0.96336       0.0029169     4      4      4             0             0.96484       0.96758       0.96596       0.00095797    4      4      4             0         
INFO:pipegenie_evolution:23     118       0.9529        0.96758       0.96348       0.0031899     3      4      3.995         0.070534      0.96484       0.96758       0.96607       0.00098433    4      4      4             0         
INFO:pipegenie_evolution:24     128       0.93676       0.96758       0.96357       0.0039864     4      4      4             0             0.96484       0.96758       0.96615       0.0010375     4      4      4             0         
INFO:pipegenie_evolution:25     128       0.94848       0.96758       0.96384       0.0033501     4      4      4             0             0.96484       0.96758       0.96634       0.0011011     4      4      4             0         
INFO:pipegenie_evolution:26     115       0.93781       0.96857       0.96437       0.004002      4      4      4             0             0.96484       0.96857       0.96671       0.0011572     4      4      4             0         
INFO:pipegenie_evolution:27     126       0.93437       0.96857       0.96402       0.0041923     3      4      3.995         0.070534      0.96484       0.96857       0.96687       0.0011591     4      4      4             0         
INFO:pipegenie_evolution:28     122       0.95094       0.96857       0.96406       0.0039945     4      4      4             0             0.96484       0.96857       0.967         0.0010641     4      4      4             0         
INFO:pipegenie_evolution:29     128       0.94896       0.96857       0.96477       0.0038366     3      4      3.995         0.070534      0.96484       0.96857       0.96707       0.0010694     4      4      4             0         
INFO:pipegenie_evolution:30     111       0.92664       0.96857       0.96444       0.0065231     4      4      4             0             0.96484       0.96857       0.96723       0.0010208     4      4      4             0         
INFO:pipegenie_evolution:31     110       0.95192       0.96857       0.96537       0.0037169     3      4      3.995         0.070534      0.96484       0.96857       0.96726       0.0010542     4      4      4             0         
INFO:pipegenie_evolution:32     109       0.94329       0.96857       0.96541       0.0046065     4      4      4             0             0.96484       0.96857       0.96745       0.00093141    4      4      4             0         
INFO:pipegenie_evolution:33     120       0.93453       0.96857       0.96511       0.0055339     3      4      3.995         0.070534      0.96484       0.96857       0.96767       0.00082675    4      4      4             0         
INFO:pipegenie_evolution:34     132       0.94486       0.96857       0.9654        0.0045651     4      4      4             0             0.96484       0.96857       0.96772       0.00084889    4      4      4             0         
INFO:pipegenie_evolution:35     113       0.94384       0.96857       0.96679       0.0033005     3      4      3.985         0.12155       0.96484       0.96857       0.96775       0.00084395    4      4      4             0         
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 17:36:32
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:27.642136
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_288
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 17:36:33
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 275.

‚ñ∂Ô∏è Processing OpenML Task ID: 288 ---
   Dataset: waveform-5000, Shape: 5000.0x41.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.33333       0.86389       0.78507       0.12214       3      4      3.745         0.43586       0.84875       0.86126       0.85362       0.0033794     3      4      3.7           0.45826   
INFO:pipegenie_evolution:1      150       0.33333       0.86593       0.80109       0.096143      3      4      3.765         0.424         0.84416       0.86593       0.85454       0.0049079     3      4      3.7           0.45826   
INFO:pipegenie_evolution:2      149       0.33333       0.86593       0.82731       0.058468      3      4      3.76          0.42708       0.85215       0.86593       0.85819       0.0039194     3      4      3.75          0.43301   
INFO:pipegenie_evolution:3      128       0.72387       0.86644       0.8442        0.018205      3      4      3.8           0.4           0.85441       0.86644       0.85962       0.0038516     3      4      3.7           0.45826   
INFO:pipegenie_evolution:4      139       0.81776       0.86644       0.85322       0.007953      3      4      3.74          0.43863       0.85441       0.86644       0.86142       0.0039901     3      4      3.75          0.43301   
INFO:pipegenie_evolution:5      137       0.83164       0.86644       0.85657       0.006574      3      4      3.705         0.45604       0.85441       0.86644       0.86352       0.0037912     3      4      3.6           0.4899    
INFO:pipegenie_evolution:6      137       0.83498       0.8665        0.8588        0.0054585     3      4      3.7           0.45826       0.85441       0.8665        0.86352       0.0037939     3      4      3.65          0.47697   
INFO:pipegenie_evolution:7      133       0.81687       0.8665        0.86095       0.0053655     3      4      3.72          0.449         0.85441       0.8665        0.86361       0.0037976     3      4      3.65          0.47697   
INFO:pipegenie_evolution:8      134       0.84221       0.8665        0.86255       0.0042441     3      4      3.765         0.424         0.85441       0.8665        0.86372       0.0038649     3      4      3.65          0.47697   
INFO:pipegenie_evolution:9      132       0.84843       0.8668        0.86386       0.0035928     3      4      3.825         0.37997       0.85441       0.8665        0.86409       0.0036722     3      4      3.7           0.45826   
INFO:pipegenie_evolution:10     149       0.85441       0.86832       0.86485       0.0025271     3      4      3.805         0.3962        0.85441       0.86832       0.86418       0.0037521     3      4      3.7           0.45826   
INFO:pipegenie_evolution:11     143       0.83734       0.86832       0.86533       0.0029243     3      4      3.675         0.46837       0.85441       0.86832       0.86458       0.00375       3      4      3.7           0.45826   
INFO:pipegenie_evolution:12     131       0.84816       0.86832       0.86538       0.0028778     3      4      3.595         0.49089       0.85441       0.86832       0.86458       0.003741      3      4      3.8           0.4       
INFO:pipegenie_evolution:13     140       0.85373       0.86832       0.86541       0.0027084     3      4      3.62          0.48539       0.85441       0.86832       0.86533       0.0030315     3      4      3.8           0.4       
INFO:pipegenie_evolution:14     129       0.85441       0.86832       0.86612       0.0018894     3      4      3.605         0.48885       0.85441       0.86832       0.86533       0.0030315     3      4      3.8           0.4       
INFO:pipegenie_evolution:15     115       0.85441       0.86832       0.86629       0.0017966     3      4      3.74          0.43863       0.85441       0.86832       0.86528       0.0029955     3      4      3.8           0.4       
INFO:pipegenie_evolution:16     124       0.83957       0.8685        0.86619       0.0030182     3      4      3.935         0.24653       0.85441       0.8685        0.86535       0.0030532     3      4      3.8           0.4       
INFO:pipegenie_evolution:17     109       0.85441       0.8685        0.86696       0.0012252     3      4      3.97          0.17059       0.85441       0.8685        0.86545       0.003118      3      4      3.8           0.4       
INFO:pipegenie_evolution:18     121       0.84568       0.8685        0.86687       0.0024338     3      4      3.98          0.14          0.85441       0.8685        0.86545       0.003118      3      4      3.8           0.4       
INFO:pipegenie_evolution:19     119       0.83805       0.86875       0.86693       0.0036748     3      4      3.975         0.15612       0.85441       0.8685        0.86545       0.003118      3      4      3.8           0.4       
INFO:pipegenie_evolution:20     92        0.85227       0.86875       0.86761       0.0019606     3      4      3.98          0.14          0.85441       0.8685        0.8657        0.003241      3      4      3.8           0.4       
INFO:pipegenie_evolution:21     108       0.85441       0.86875       0.86784       0.0014684     3      4      3.985         0.12155       0.85441       0.8685        0.8656        0.0031833     3      4      3.8           0.4       
INFO:pipegenie_evolution:22     115       0.85432       0.86875       0.86781       0.0019575     3      4      3.985         0.12155       0.85441       0.8685        0.8656        0.0031833     3      4      3.8           0.4       
INFO:pipegenie_evolution:23     104       0.85441       0.86875       0.86784       0.0017977     3      4      3.985         0.12155       0.85441       0.8685        0.8656        0.0031833     3      4      3.8           0.4       
INFO:pipegenie_evolution:24     120       0.83603       0.86875       0.86673       0.0054389     3      4      3.985         0.12155       0.85441       0.8685        0.8656        0.0031833     3      4      3.8           0.4       
INFO:pipegenie_evolution:25     120       0.85441       0.86875       0.86801       0.0015171     3      4      3.98          0.14          0.85441       0.8685        0.8656        0.0031833     3      4      3.8           0.4       
INFO:pipegenie_evolution:26     117       0.8517        0.86973       0.86769       0.0026037     3      4      3.98          0.14          0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:27     113       0.85142       0.86943       0.86788       0.0023467     3      4      3.98          0.14          0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:28     127       0.86125       0.86943       0.86819       0.0012833     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:29     121       0.85812       0.86943       0.86811       0.0015653     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:30     115       0.85256       0.86943       0.86793       0.002274      3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:31     114       0.83225       0.86943       0.86703       0.0059154     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:32     115       0.8517        0.86943       0.86818       0.0021222     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:33     121       0.85026       0.86943       0.86821       0.0022751     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:34     136       0.84213       0.86943       0.86787       0.0035911     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:35     130       0.85168       0.86943       0.86815       0.0029578     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:36     125       0.85441       0.86973       0.86871       0.0016602     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:37     136       0.85199       0.86974       0.8685        0.0023216     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:38     126       0.83224       0.86973       0.86824       0.0043756     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:39     122       0.8555        0.86973       0.86868       0.0022845     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:40     131       0.8623        0.86973       0.8691        0.0011408     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:41     131       0.84746       0.86973       0.86883       0.0023931     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:42     131       0.85441       0.86973       0.86911       0.001758      3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:43     121       0.84794       0.86973       0.86867       0.0032248     3      4      3.98          0.14          0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:44     133       0.83422       0.86973       0.86841       0.0043175     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:45     107       0.85441       0.86973       0.869         0.0018096     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:46     124       0.85441       0.86973       0.86902       0.001965      3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:47     111       0.84072       0.86973       0.8686        0.0035755     3      4      3.98          0.14          0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
INFO:pipegenie_evolution:48     132       0.80694       0.86973       0.86699       0.0092455     3      4      3.985         0.12155       0.85441       0.86973       0.86567       0.0032545     3      4      3.8           0.4       
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 18:37:08
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:35.709022
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_336
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 18:37:09
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 288.

‚ñ∂Ô∏è Processing OpenML Task ID: 336 ---
   Dataset: electricity, Shape: 45312.0x9.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.49982       0.92099       0.77363       0.098173      3      4      3.73          0.44396       0.84253       0.92099       0.88774       0.025292      3      4      3.65          0.47697   
INFO:pipegenie_evolution:1      156       0.49984       0.92102       0.80057       0.083178      3      4      3.765         0.424         0.89468       0.92102       0.91118       0.0080003     3      4      3.65          0.47697   
INFO:pipegenie_evolution:2      147       0.50604       0.92202       0.83388       0.066647      3      4      3.72          0.449         0.90901       0.92202       0.91728       0.0033429     3      4      3.6           0.4899    
INFO:pipegenie_evolution:3      135       0.68595       0.9248        0.86283       0.051845      3      4      3.66          0.47371       0.91552       0.9248        0.92015       0.0024672     3      4      3.45          0.49749   
INFO:pipegenie_evolution:4      147       0.73187       0.92585       0.88867       0.040061      3      4      3.555         0.49697       0.91552       0.92585       0.92192       0.0027666     3      4      3.5           0.5       
INFO:pipegenie_evolution:5      124       0.82005       0.92768       0.90565       0.023494      3      4      3.515         0.49977       0.91552       0.92768       0.92353       0.0031371     3      4      3.7           0.45826   
INFO:pipegenie_evolution:6      151       0.87188       0.92768       0.91565       0.01126       3      4      3.47          0.4991        0.91552       0.92768       0.92432       0.0034313     3      4      3.45          0.49749   
INFO:pipegenie_evolution:7      134       0.87357       0.92948       0.91979       0.0074996     3      4      3.425         0.49434       0.9162        0.92948       0.92582       0.0032338     3      4      3.5           0.5       
INFO:pipegenie_evolution:8      124       0.89295       0.92948       0.92256       0.0049929     3      4      3.39          0.48775       0.91925       0.92948       0.92728       0.0025437     3      4      3.35          0.47697   
INFO:pipegenie_evolution:9      141       0.91279       0.92948       0.92469       0.0030361     3      4      3.31          0.46249       0.91925       0.92948       0.92764       0.0025923     3      4      3.35          0.47697   
INFO:pipegenie_evolution:10     121       0.80644       0.92948       0.92365       0.011736      3      4      3.33          0.47021       0.91925       0.92948       0.92775       0.0026228     3      4      3.3           0.45826   
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 19:40:36
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:03:27.399952
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_340
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 19:40:37
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 336.

‚ñ∂Ô∏è Processing OpenML Task ID: 340 ---
   Dataset: pokerhand, Shape: 829201.0x11.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.12574       0.78972       0.35941       0.19601       3      4      3.74          0.43863       0.17962       0.78972       0.38224       0.17295       3      4      3.65          0.47697   
INFO:pipegenie_evolution:1      190       0.12574       0.78972       0.36854       0.17655       3      4      3.78          0.41425       0.24719       0.78972       0.4473        0.1686        3      4      3.7           0.45826   
INFO:pipegenie_evolution:2      186       0.14033       0.83976       0.41519       0.18825       3      4      3.78          0.41425       0.31884       0.83976       0.53242       0.16883       3      4      3.75          0.43301   
INFO:pipegenie_evolution:3      180       0.13165       0.83976       0.42212       0.17131       3      4      3.755         0.43009       0.33447       0.83976       0.57806       0.13051       3      4      3.55          0.49749   
INFO:pipegenie_evolution:4      179       0.1325        0.83976       0.44297       0.17808       3      4      3.735         0.44133       0.5207        0.83976       0.61133       0.09515       3      4      3.35          0.47697   
INFO:pipegenie_evolution:5      179       0.12575       0.83976       0.46058       0.17965       3      4      3.695         0.46041       0.52566       0.83976       0.63395       0.087778      3      4      3.5           0.5       
INFO:pipegenie_evolution:6      177       0.17487       0.83976       0.50809       0.17115       3      4      3.67          0.47021       0.52566       0.83976       0.63469       0.086928      3      4      3.5           0.5       
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 20:45:45
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:05:08.009413
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2119
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 20:45:45
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 340.

‚ñ∂Ô∏è Processing OpenML Task ID: 2119 ---
   Dataset: yeast, Shape: 1484.0x9.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.099066      0.57986       0.42231       0.12392       3      4      3.74          0.43863       0.54544       0.57986       0.55934       0.0093549     3      4      3.5           0.5       
INFO:pipegenie_evolution:1      147       0.099066      0.57986       0.45066       0.10417       3      4      3.765         0.424         0.54392       0.57986       0.56246       0.010075      3      4      3.85          0.35707   
INFO:pipegenie_evolution:2      139       0.1           0.60115       0.49167       0.08702       3      4      3.7           0.45826       0.54392       0.60115       0.56955       0.015165      3      4      3.9           0.3       
INFO:pipegenie_evolution:3      143       0.17303       0.61025       0.52358       0.062126      3      4      3.75          0.43301       0.54392       0.61025       0.58005       0.014626      3      4      3.9           0.3       
INFO:pipegenie_evolution:4      136       0.39265       0.61025       0.54617       0.040645      3      4      3.75          0.43301       0.54642       0.61025       0.58059       0.016812      3      4      3.9           0.3       
INFO:pipegenie_evolution:5      121       0.46747       0.61025       0.56707       0.023022      3      4      3.805         0.3962        0.54642       0.61025       0.589         0.015416      3      4      3.95          0.21794   
INFO:pipegenie_evolution:6      150       0.44568       0.61025       0.57237       0.028017      3      4      3.885         0.31902       0.54642       0.61025       0.5912        0.018092      4      4      4             0         
INFO:pipegenie_evolution:7      140       0.45566       0.61381       0.58274       0.024404      3      4      3.955         0.2073        0.54891       0.61381       0.59517       0.015888      4      4      4             0         
INFO:pipegenie_evolution:8      131       0.52412       0.61381       0.59116       0.017532      3      4      3.97          0.17059       0.56541       0.61381       0.59599       0.013748      3      4      3.95          0.21794   
INFO:pipegenie_evolution:9      131       0.47432       0.61381       0.59694       0.020529      3      4      3.98          0.14          0.56541       0.61381       0.59599       0.013748      3      4      3.95          0.21794   
INFO:pipegenie_evolution:10     111       0.54911       0.61381       0.60384       0.011507      3      4      3.97          0.17059       0.56541       0.61381       0.59599       0.013748      3      4      3.95          0.21794   
INFO:pipegenie_evolution:11     112       0.51108       0.62047       0.60567       0.01202       3      4      3.975         0.15612       0.57986       0.62047       0.59874       0.012831      4      4      4             0         
INFO:pipegenie_evolution:12     113       0.54884       0.62047       0.60796       0.0086439     3      4      3.97          0.17059       0.57986       0.62047       0.59953       0.013732      3      4      3.95          0.21794   
INFO:pipegenie_evolution:13     126       0.51696       0.6282        0.60901       0.010242      3      4      3.985         0.12155       0.57986       0.6282        0.60187       0.014414      3      4      3.95          0.21794   
INFO:pipegenie_evolution:14     110       0.53203       0.6282        0.60919       0.011091      3      4      3.985         0.12155       0.57986       0.6282        0.60216       0.014632      3      4      3.95          0.21794   
INFO:pipegenie_evolution:15     108       0.58017       0.6282        0.61154       0.0054948     3      4      3.985         0.12155       0.57986       0.6282        0.60216       0.014632      3      4      3.95          0.21794   
INFO:pipegenie_evolution:16     120       0.5784        0.6282        0.61224       0.0056994     4      4      4             0             0.57986       0.6282        0.60239       0.014878      3      4      3.95          0.21794   
INFO:pipegenie_evolution:17     97        0.58017       0.6282        0.61283       0.0056135     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:18     132       0.58017       0.6282        0.61282       0.0064322     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:19     124       0.58017       0.6282        0.61328       0.0063557     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:20     132       0.56455       0.6282        0.61262       0.0077258     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:21     113       0.55572       0.6282        0.61312       0.0083342     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:22     143       0.57954       0.6282        0.61303       0.0079774     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:23     143       0.58017       0.6282        0.61335       0.0077463     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:24     146       0.54518       0.6282        0.6131        0.0089054     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:25     143       0.58017       0.6282        0.61489       0.0073581     4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:26     134       0.55335       0.6282        0.61356       0.011384      4      4      4             0             0.57986       0.6282        0.60343       0.016198      4      4      4             0         
INFO:pipegenie_evolution:27     128       0.58017       0.6282        0.616         0.0075492     4      4      4             0             0.57986       0.6282        0.60778       0.017195      4      4      4             0         
INFO:pipegenie_evolution:28     125       0.58017       0.6282        0.61637       0.008296      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:29     137       0.57196       0.6282        0.61573       0.0104        4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:30     137       0.53035       0.6282        0.61589       0.012723      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:31     134       0.57049       0.6282        0.61559       0.010679      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:32     134       0.579         0.6282        0.61668       0.01094       4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:33     116       0.58017       0.6282        0.61859       0.01149       4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:34     129       0.51049       0.6282        0.61892       0.015668      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:35     129       0.5572        0.6282        0.6213        0.012066      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:36     116       0.56231       0.6282        0.62362       0.0116        4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:37     114       0.58017       0.6282        0.62522       0.0084647     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:38     98        0.53367       0.6282        0.62483       0.011841      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:39     110       0.58017       0.6282        0.62598       0.00787       4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:40     109       0.58017       0.6282        0.62627       0.0075791     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:41     122       0.58017       0.6282        0.62598       0.007873      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:42     94        0.58017       0.6282        0.62664       0.0070683     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:43     110       0.56256       0.6282        0.62531       0.010572      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:44     120       0.58017       0.6282        0.62581       0.0082716     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:45     115       0.47897       0.6282        0.62259       0.020509      3      4      3.995         0.070534      0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:46     109       0.58017       0.6282        0.62637       0.0074241     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:47     104       0.57647       0.6282        0.62547       0.0094026     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:48     127       0.57298       0.6282        0.62511       0.0099157     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:49     128       0.44497       0.6282        0.62251       0.021239      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:50     111       0.58017       0.6282        0.62591       0.0083805     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:51     119       0.57994       0.6282        0.62553       0.008822      4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:52     105       0.5027        0.6282        0.6241        0.014237      3      4      3.995         0.070534      0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:53     100       0.58017       0.6282        0.62594       0.0079376     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:54     102       0.58017       0.6282        0.62582       0.0087803     4      4      4             0             0.58017       0.6282        0.61013       0.016413      4      4      4             0         
INFO:pipegenie_evolution:55     126       0.56352       0.6282        0.62528       0.010185      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:56     126       0.58017       0.6282        0.62483       0.009488      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:57     112       0.58017       0.6282        0.62597       0.0078793     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:58     103       0.56038       0.6282        0.62546       0.0099762     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:59     102       0.56652       0.6282        0.62469       0.011171      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:60     98        0.52636       0.6282        0.62403       0.014448      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:61     110       0.58017       0.6282        0.62631       0.0074035     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:62     112       0.58017       0.6282        0.62594       0.0078084     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:63     119       0.57128       0.6282        0.6254        0.0095268     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:64     118       0.54368       0.6282        0.62408       0.013177      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:65     117       0.58017       0.6282        0.62673       0.0068523     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:66     113       0.57049       0.6282        0.62501       0.010652      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:67     122       0.58017       0.6282        0.62618       0.0076608     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:68     133       0.50161       0.6282        0.62487       0.013388      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:69     123       0.55352       0.6282        0.62544       0.010826      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:70     114       0.57128       0.6282        0.62506       0.0098677     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:71     124       0.52591       0.6282        0.6255        0.010902      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:72     123       0.58017       0.6282        0.62589       0.0083515     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:73     125       0.58017       0.6282        0.62634       0.0076739     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:74     122       0.55321       0.6282        0.62432       0.011925      3      4      3.995         0.070534      0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:75     128       0.52875       0.6282        0.62427       0.01268       4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:76     133       0.58017       0.6282        0.62606       0.0078517     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:77     119       0.58017       0.6282        0.62653       0.0070768     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:78     130       0.49528       0.6282        0.62287       0.018528      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:79     134       0.58017       0.6282        0.62501       0.0090889     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:80     128       0.43743       0.6282        0.62037       0.02951       3      4      3.99          0.099499      0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:81     120       0.58017       0.6282        0.62672       0.0068395     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:82     124       0.58017       0.6282        0.62577       0.0082146     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:83     119       0.58017       0.6282        0.62496       0.0091745     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:84     125       0.46172       0.6282        0.62186       0.021456      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:85     113       0.50896       0.6282        0.62446       0.012484      3      4      3.995         0.070534      0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:86     124       0.48941       0.6282        0.62429       0.016564      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:87     126       0.55127       0.6282        0.62444       0.011677      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:88     118       0.57049       0.6282        0.62539       0.009349      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:89     105       0.58017       0.6282        0.62539       0.0090696     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:90     118       0.58017       0.6282        0.6246        0.0099668     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:91     131       0.58017       0.6282        0.62506       0.0092677     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:92     111       0.58017       0.6282        0.62569       0.0084141     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:93     97        0.58017       0.6282        0.62621       0.007724      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:94     97        0.58017       0.6282        0.62652       0.0071768     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:95     110       0.56505       0.6282        0.62529       0.010161      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:96     90        0.58017       0.6282        0.62641       0.0075157     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:97     121       0.5399        0.6282        0.62589       0.0096988     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:98     108       0.58017       0.6282        0.62644       0.0073252     4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:99     125       0.5453        0.6282        0.62467       0.012018      4      4      4             0             0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:100    112       0.57884       0.6282        0.62526       0.0096776     3      4      3.995         0.070534      0.58017       0.6282        0.60889       0.015931      4      4      4             0         
INFO:pipegenie_evolution:101    111       0.58017       0.6282        0.62584       0.0081487     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:102    119       0.58017       0.6282        0.62617       0.0076214     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:103    120       0.57049       0.6282        0.62517       0.010002      4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:104    132       0.57049       0.6282        0.62566       0.0091356     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:105    123       0.58017       0.6282        0.62515       0.0096615     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:106    109       0.57298       0.6282        0.6247        0.010251      4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:107    120       0.56256       0.6282        0.62462       0.011275      4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:108    97        0.58017       0.6282        0.62614       0.0075617     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:109    102       0.46444       0.6282        0.62297       0.019545      4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:110    100       0.58017       0.6282        0.62484       0.0093584     4      4      4             0             0.58017       0.6282        0.60923       0.016314      4      4      4             0         
INFO:pipegenie_evolution:111    114       0.58017       0.6282        0.62567       0.0079857     4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:112    99        0.58017       0.6282        0.62577       0.0080271     4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:113    95        0.51501       0.6282        0.62483       0.014703      4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:114    91        0.53449       0.6282        0.62562       0.010112      4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:115    107       0.58017       0.6282        0.62516       0.0091247     4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:116    112       0.45904       0.6282        0.62232       0.023238      4      4      4             0             0.58017       0.6282        0.60902       0.01609       4      4      4             0         
INFO:pipegenie_evolution:117    105       0.5685        0.6282        0.62502       0.010088      3      4      3.995         0.070534      0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:118    126       0.47254       0.6282        0.62309       0.017787      4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:119    101       0.58017       0.6282        0.6255        0.0086953     4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:120    83        0.58017       0.6282        0.62662       0.0069691     4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:121    105       0.54408       0.6282        0.62482       0.012565      4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:122    102       0.58017       0.6282        0.62557       0.0085946     4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:123    92        0.58017       0.6282        0.6264        0.0072938     4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:124    87        0.57049       0.6282        0.62558       0.0093816     4      4      4             0             0.58017       0.6282        0.60724       0.016311      3      4      3.95          0.21794   
INFO:pipegenie_evolution:125    107       0.58017       0.6282        0.62616       0.0077968     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:126    104       0.58017       0.6282        0.62546       0.0089754     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:127    104       0.58017       0.6282        0.62639       0.0074859     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:128    109       0.58017       0.6282        0.62616       0.0075449     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:129    121       0.58017       0.6282        0.62563       0.0084862     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:130    119       0.5573        0.6282        0.62437       0.011567      4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:131    108       0.58017       0.6282        0.62576       0.0082123     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:132    121       0.58017       0.6282        0.62602       0.0078858     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:133    121       0.58017       0.6282        0.62557       0.0086265     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:134    116       0.58017       0.6282        0.62616       0.0077984     4      4      4             0             0.58017       0.6282        0.60897       0.016049      4      4      4             0         
INFO:pipegenie_evolution:135    92        0.58017       0.6282        0.62633       0.0074461     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:136    124       0.58017       0.6282        0.62615       0.0077241     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:137    110       0.58017       0.6282        0.62626       0.0077639     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:138    112       0.58017       0.6282        0.626         0.0083153     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:139    113       0.44983       0.6282        0.62201       0.026904      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:140    108       0.58017       0.6282        0.62545       0.0087993     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:141    110       0.58017       0.6282        0.62641       0.0073685     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:142    111       0.58017       0.6282        0.62635       0.0073147     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:143    128       0.58017       0.6282        0.62604       0.0077171     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:144    115       0.58017       0.6282        0.6255        0.0086028     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:145    117       0.45511       0.6282        0.61936       0.029408      3      4      3.99          0.099499      0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:146    117       0.575         0.6282        0.62436       0.011202      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:147    139       0.48543       0.6282        0.6243        0.014389      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:148    127       0.5153        0.6282        0.62307       0.015045      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:149    128       0.58017       0.6282        0.62591       0.0078288     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:150    102       0.58142       0.6282        0.62607       0.0074855     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:151    113       0.58017       0.6282        0.62578       0.008265      3      4      3.995         0.070534      0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:152    116       0.48862       0.6282        0.62396       0.016814      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:153    116       0.58017       0.6282        0.62606       0.0078366     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:154    123       0.56838       0.6282        0.62581       0.0090742     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:155    115       0.5002        0.6282        0.62356       0.017055      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:156    140       0.58017       0.6282        0.62575       0.0083783     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:157    147       0.58017       0.6282        0.62577       0.0083599     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:158    116       0.57647       0.6282        0.62547       0.0093875     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:159    125       0.54056       0.6282        0.6255        0.010271      3      4      3.995         0.070534      0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:160    131       0.58017       0.6282        0.62626       0.007585      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:161    124       0.58017       0.6282        0.62535       0.0089611     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:162    125       0.55172       0.6282        0.62504       0.010604      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:163    124       0.58017       0.6282        0.62546       0.0090833     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:164    118       0.58017       0.6282        0.62539       0.0088982     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:165    127       0.58017       0.6282        0.6259        0.0083146     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:166    133       0.4559        0.6282        0.62222       0.023391      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:167    116       0.55593       0.6282        0.62527       0.010494      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:168    136       0.58017       0.6282        0.62523       0.0091518     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:169    124       0.58017       0.6282        0.62653       0.0071817     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:170    139       0.57895       0.6282        0.62521       0.0095443     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:171    117       0.57049       0.6282        0.6254        0.0095901     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:172    134       0.55656       0.6282        0.6249        0.011227      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:173    128       0.58017       0.6282        0.6254        0.0090075     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:174    121       0.48775       0.6282        0.62317       0.019065      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:175    126       0.58017       0.6282        0.62613       0.0076718     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:176    131       0.58017       0.6282        0.62636       0.0073429     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:177    129       0.57049       0.6282        0.62436       0.011926      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:178    111       0.58017       0.6282        0.62601       0.0078324     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:179    122       0.58017       0.6282        0.62609       0.0077851     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:180    129       0.58017       0.6282        0.62567       0.008822      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:181    123       0.575         0.6282        0.62535       0.0094579     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:182    143       0.56111       0.6282        0.62567       0.0091359     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:183    141       0.58017       0.6282        0.62509       0.0089535     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:184    129       0.58017       0.6282        0.6256        0.0086325     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:185    126       0.58017       0.6282        0.62628       0.0075731     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:186    145       0.51604       0.6282        0.62494       0.013438      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:187    133       0.58017       0.6282        0.62666       0.006946      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:188    128       0.58017       0.6282        0.62596       0.0080621     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:189    133       0.50785       0.6282        0.62431       0.012359      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:190    138       0.58017       0.6282        0.62587       0.0083021     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:191    134       0.43648       0.6282        0.62079       0.026921      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:192    134       0.57049       0.6282        0.62486       0.010167      3      4      3.995         0.070534      0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:193    145       0.57049       0.6282        0.62466       0.010621      4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:194    132       0.58017       0.6282        0.62569       0.0086291     4      4      4             0             0.58017       0.6282        0.60956       0.016156      4      4      4             0         
INFO:pipegenie_evolution:195    126       0.56367       0.6282        0.6243        0.011816      4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:196    137       0.58017       0.6282        0.62578       0.0082554     4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:197    118       0.58017       0.6282        0.62573       0.0084835     4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:198    140       0.47018       0.6282        0.62439       0.017505      4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:199    131       0.46856       0.6282        0.62324       0.019827      4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:200    135       0.57049       0.6282        0.62453       0.010229      4      4      4             0             0.5662        0.6282        0.60646       0.018112      4      4      4             0         
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 21:42:47
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 0:57:01.753080
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2120
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 21:42:47
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2119.

‚ñ∂Ô∏è Processing OpenML Task ID: 2120 ---
   Dataset: satimage, Shape: 6430.0x37.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.16667       0.89901       0.80676       0.1365        3      4      3.755         0.43009       0.88894       0.89901       0.89185       0.0024178     3      4      3.85          0.35707   
INFO:pipegenie_evolution:1      148       0.16667       0.90227       0.82972       0.11826       3      4      3.795         0.4037        0.88916       0.90227       0.89427       0.0030802     3      4      3.85          0.35707   
INFO:pipegenie_evolution:2      136       0.51184       0.90227       0.86328       0.053604      3      4      3.765         0.424         0.88916       0.90227       0.8957        0.0028673     3      4      3.7           0.45826   
INFO:pipegenie_evolution:3      135       0.70992       0.90244       0.88001       0.030399      3      4      3.745         0.43586       0.88916       0.90244       0.89694       0.0035227     3      4      3.75          0.43301   
INFO:pipegenie_evolution:4      136       0.86716       0.90244       0.89185       0.0057885     3      4      3.77          0.42083       0.88916       0.90244       0.89754       0.0036344     3      4      3.75          0.43301   
INFO:pipegenie_evolution:5      141       0.88126       0.90244       0.89389       0.00378       3      4      3.82          0.38419       0.88916       0.90244       0.89811       0.0035565     3      4      3.7           0.45826   
INFO:pipegenie_evolution:6      146       0.88664       0.90244       0.89462       0.0032321     3      4      3.85          0.35707       0.89096       0.90244       0.8992        0.0024365     3      4      3.75          0.43301   
INFO:pipegenie_evolution:7      138       0.88776       0.90244       0.89543       0.0029096     3      4      3.82          0.38419       0.89096       0.90244       0.8992        0.0030317     3      4      3.7           0.45826   
INFO:pipegenie_evolution:8      151       0.85604       0.90244       0.89517       0.0050176     3      4      3.845         0.3619        0.89096       0.90244       0.89973       0.0031781     3      4      3.75          0.43301   
INFO:pipegenie_evolution:9      136       0.87722       0.90252       0.89615       0.0035763     3      4      3.84          0.36661       0.89096       0.90252       0.89983       0.0031922     3      4      3.75          0.43301   
INFO:pipegenie_evolution:10     142       0.88611       0.90362       0.89664       0.0031922     3      4      3.84          0.36661       0.89096       0.90362       0.90008       0.0032325     3      4      3.7           0.45826   
INFO:pipegenie_evolution:11     147       0.87504       0.90362       0.8965        0.0037833     3      4      3.86          0.34699       0.89096       0.90362       0.90018       0.0032232     3      4      3.7           0.45826   
INFO:pipegenie_evolution:12     144       0.88633       0.90362       0.89632       0.003406      3      4      3.87          0.3363        0.89096       0.90362       0.90065       0.0024381     3      4      3.7           0.45826   
INFO:pipegenie_evolution:13     134       0.88848       0.90362       0.89682       0.0033485     3      4      3.88          0.32496       0.89096       0.90362       0.90062       0.0024373     3      4      3.7           0.45826   
INFO:pipegenie_evolution:14     129       0.888         0.90362       0.89685       0.0035387     3      4      3.87          0.3363        0.89096       0.90362       0.9008        0.0024598     3      4      3.7           0.45826   
INFO:pipegenie_evolution:15     150       0.88522       0.90362       0.89689       0.0035948     3      4      3.915         0.27888       0.89096       0.90362       0.90085       0.0024849     3      4      3.7           0.45826   
INFO:pipegenie_evolution:16     124       0.88775       0.90362       0.89749       0.0034481     3      4      3.91          0.28618       0.89096       0.90362       0.9009        0.0025263     3      4      3.7           0.45826   
INFO:pipegenie_evolution:17     125       0.88935       0.90362       0.89771       0.003196      3      4      3.945         0.22798       0.89096       0.90362       0.9009        0.0025264     3      4      3.7           0.45826   
INFO:pipegenie_evolution:18     129       0.89096       0.90362       0.89789       0.0033905     3      4      3.925         0.26339       0.89096       0.90362       0.90103       0.0025871     3      4      3.7           0.45826   
INFO:pipegenie_evolution:19     124       0.87448       0.90362       0.89764       0.0046753     3      4      3.91          0.28618       0.89096       0.90362       0.90139       0.0026782     3      4      3.75          0.43301   
INFO:pipegenie_evolution:20     127       0.8702        0.90362       0.89811       0.0050512     3      4      3.925         0.26339       0.89096       0.90362       0.90144       0.0026718     3      4      3.75          0.43301   
INFO:pipegenie_evolution:21     119       0.89012       0.90362       0.89919       0.0037561     3      4      3.93          0.25515       0.89096       0.90362       0.90151       0.002684      3      4      3.75          0.43301   
INFO:pipegenie_evolution:22     107       0.88819       0.90362       0.89946       0.0039964     3      4      3.95          0.21794       0.89096       0.90362       0.90164       0.0026567     3      4      3.75          0.43301   
INFO:pipegenie_evolution:23     102       0.89052       0.90362       0.8998        0.0038186     3      4      3.935         0.24653       0.89096       0.90362       0.90164       0.0026567     3      4      3.75          0.43301   
INFO:pipegenie_evolution:24     112       0.89061       0.90362       0.90018       0.0036706     3      4      3.94          0.23749       0.89096       0.90362       0.90164       0.0026567     3      4      3.75          0.43301   
INFO:pipegenie_evolution:25     98        0.88856       0.90362       0.89972       0.0042314     3      4      3.95          0.21794       0.89096       0.90362       0.90171       0.0026676     3      4      3.8           0.4       
INFO:pipegenie_evolution:26     111       0.88963       0.90517       0.8995        0.0042698     3      4      3.94          0.23749       0.89096       0.90517       0.90188       0.002732      3      4      3.8           0.4       
INFO:pipegenie_evolution:27     117       0.89096       0.90555       0.89965       0.0042134     3      4      3.955         0.2073        0.89096       0.90555       0.90213       0.0029122     3      4      3.8           0.4       
INFO:pipegenie_evolution:28     119       0.8893        0.90555       0.89977       0.0041706     3      4      3.98          0.14          0.89096       0.90555       0.90219       0.0029228     3      4      3.8           0.4       
INFO:pipegenie_evolution:29     118       0.89105       0.90555       0.89981       0.0041831     3      4      3.975         0.15612       0.8999        0.90555       0.90314       0.0015873     3      4      3.85          0.35707   
INFO:pipegenie_evolution:30     120       0.86837       0.90555       0.90019       0.0062258     3      4      3.98          0.14          0.8999        0.90555       0.90314       0.0015873     3      4      3.85          0.35707   
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 22:43:50
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:01:02.312838
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2121
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 22:43:50
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2120.

‚ñ∂Ô∏è Processing OpenML Task ID: 2121 ---
   Dataset: abalone, Shape: 4177.0x9.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.046834      0.2019        0.13801       0.025729      3      4      3.745         0.43586       0.15847       0.2019        0.17101       0.010979      3      4      3.85          0.35707   
INFO:pipegenie_evolution:1      162       0.046834      0.21555       0.1429        0.024544      3      4      3.82          0.38419       0.15268       0.21555       0.17656       0.01632       3      4      3.75          0.43301   
INFO:pipegenie_evolution:2      145       0.061328      0.21555       0.15495       0.02051       3      4      3.82          0.38419       0.15496       0.21555       0.18449       0.014525      3      4      3.7           0.45826   
INFO:pipegenie_evolution:3      146       0.078733      0.21555       0.16136       0.021782      3      4      3.815         0.3883        0.15564       0.21555       0.18648       0.013352      3      4      3.6           0.4899    
INFO:pipegenie_evolution:4      141       0.10401       0.21555       0.17039       0.018144      3      4      3.755         0.43009       0.17884       0.21555       0.19548       0.0096622     3      4      3.6           0.4899    
INFO:pipegenie_evolution:5      133       0.12854       0.23041       0.1767        0.018784      3      4      3.7           0.45826       0.17884       0.23041       0.19842       0.013074      3      4      3.6           0.4899    
INFO:pipegenie_evolution:6      143       0.14342       0.23041       0.18273       0.01777       3      4      3.66          0.47371       0.18402       0.23041       0.20204       0.013427      3      4      3.6           0.4899    
INFO:pipegenie_evolution:7      138       0.15347       0.23041       0.18956       0.015903      3      4      3.66          0.47371       0.18402       0.23041       0.20659       0.01448       3      4      3.65          0.47697   
INFO:pipegenie_evolution:8      143       0.13838       0.23041       0.19072       0.018474      3      4      3.655         0.47537       0.18496       0.23041       0.21018       0.013316      3      4      3.7           0.45826   
INFO:pipegenie_evolution:9      138       0.13973       0.23041       0.19644       0.016539      3      4      3.625         0.48412       0.18496       0.23041       0.21          0.013744      3      4      3.8           0.4       
INFO:pipegenie_evolution:10     131       0.15819       0.23064       0.19971       0.015271      3      4      3.66          0.47371       0.18925       0.23041       0.21104       0.012542      3      4      3.75          0.43301   
INFO:pipegenie_evolution:11     140       0.15203       0.23041       0.19833       0.015483      3      4      3.67          0.47021       0.19245       0.23041       0.21236       0.011352      3      4      3.75          0.43301   
INFO:pipegenie_evolution:12     116       0.15121       0.23041       0.19974       0.01627       3      4      3.75          0.43301       0.19245       0.23041       0.2126        0.011711      3      4      3.85          0.35707   
INFO:pipegenie_evolution:13     118       0.15275       0.23041       0.20037       0.016872      3      4      3.78          0.41425       0.19245       0.23041       0.21303       0.010742      3      4      3.8           0.4       
INFO:pipegenie_evolution:14     137       0.15267       0.23041       0.20108       0.017182      3      4      3.795         0.4037        0.19245       0.23041       0.21364       0.01109       3      4      3.85          0.35707   
INFO:pipegenie_evolution:15     123       0.15727       0.23041       0.20447       0.0149        3      4      3.8           0.4           0.19245       0.23041       0.2143        0.0099947     3      4      3.85          0.35707   
INFO:pipegenie_evolution:16     134       0.16294       0.23577       0.20733       0.013274      3      4      3.815         0.3883        0.19245       0.23577       0.21592       0.01072       3      4      3.9           0.3       
INFO:pipegenie_evolution:17     118       0.14208       0.23577       0.20812       0.01577       3      4      3.8           0.4           0.19245       0.23577       0.21731       0.011145      3      4      3.9           0.3       
INFO:pipegenie_evolution:18     128       0.15623       0.23577       0.20998       0.014471      3      4      3.75          0.43301       0.19245       0.23577       0.21756       0.010988      3      4      3.95          0.21794   
INFO:pipegenie_evolution:19     110       0.1528        0.23577       0.21233       0.013539      3      4      3.725         0.44651       0.19245       0.23577       0.21756       0.010988      3      4      3.95          0.21794   
INFO:pipegenie_evolution:20     116       0.17561       0.23577       0.21425       0.012461      3      4      3.735         0.44133       0.19245       0.23577       0.21772       0.011117      3      4      3.95          0.21794   
INFO:pipegenie_evolution:21     134       0.17008       0.23711       0.21427       0.013777      3      4      3.72          0.449         0.19245       0.23711       0.22031       0.012446      3      4      3.9           0.3       
INFO:pipegenie_evolution:22     98        0.16369       0.23711       0.21577       0.012851      3      4      3.645         0.47851       0.19245       0.23646       0.21875       0.012199      3      4      3.9           0.3       
INFO:pipegenie_evolution:23     110       0.16107       0.23711       0.2155        0.014473      3      4      3.695         0.46041       0.19245       0.23646       0.22008       0.012175      3      4      3.85          0.35707   
INFO:pipegenie_evolution:24     107       0.16458       0.23825       0.21679       0.014037      3      4      3.735         0.44133       0.19245       0.23646       0.21988       0.012002      3      4      3.9           0.3       
INFO:pipegenie_evolution:25     134       0.17024       0.23711       0.21752       0.013654      3      4      3.745         0.43586       0.19245       0.23646       0.21988       0.012002      3      4      3.9           0.3       
INFO:pipegenie_evolution:26     120       0.18668       0.23825       0.21876       0.012692      3      4      3.785         0.41082       0.19245       0.23646       0.22016       0.012256      3      4      3.9           0.3       
INFO:pipegenie_evolution:27     100       0.14823       0.23711       0.21857       0.017954      3      4      3.81          0.3923        0.19245       0.23646       0.22016       0.012256      3      4      3.9           0.3       
INFO:pipegenie_evolution:28     115       0.16769       0.23711       0.22065       0.015419      3      4      3.8           0.4           0.19245       0.23646       0.22016       0.012256      3      4      3.9           0.3       
INFO:pipegenie_evolution:29     128       0.14982       0.23742       0.22128       0.01638       3      4      3.87          0.3363        0.19245       0.23582       0.22039       0.012492      3      4      3.9           0.3       
INFO:pipegenie_evolution:30     121       0.15849       0.23742       0.22267       0.014649      3      4      3.885         0.31902       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:31     112       0.1493        0.23711       0.22162       0.020625      3      4      3.91          0.28618       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:32     101       0.16181       0.23711       0.22753       0.013942      3      4      3.905         0.29321       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:33     99        0.15925       0.23711       0.2294        0.012783      3      4      3.925         0.26339       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:34     102       0.14955       0.23711       0.22923       0.015211      3      4      3.915         0.27888       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:35     128       0.14861       0.23711       0.22944       0.016461      3      4      3.965         0.18378       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:36     116       0.1899        0.23711       0.23242       0.0088069     3      4      3.97          0.17059       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:37     104       0.16485       0.23711       0.23224       0.010102      3      4      3.985         0.12155       0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:38     88        0.17179       0.23711       0.23299       0.0088345     3      4      3.99          0.099499      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:39     94        0.14785       0.23711       0.23128       0.014747      3      4      3.995         0.070534      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:40     90        0.15317       0.23711       0.23138       0.014434      3      4      3.995         0.070534      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:41     90        0.18219       0.23711       0.2335        0.0082113     3      4      3.995         0.070534      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:42     68        0.18196       0.23711       0.23403       0.0081161     3      4      3.99          0.099499      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:43     75        0.18709       0.23711       0.23392       0.0075239     3      4      3.995         0.070534      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:44     80        0.19596       0.23711       0.23352       0.0077337     3      4      3.99          0.099499      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:45     87        0.17701       0.23711       0.23207       0.0099731     3      4      3.99          0.099499      0.19245       0.23577       0.22036       0.01245       3      4      3.9           0.3       
INFO:pipegenie_evolution:46     83        0.16392       0.24028       0.23162       0.012009      3      4      3.99          0.099499      0.19245       0.24028       0.22061       0.012801      3      4      3.9           0.3       
INFO:pipegenie_evolution:47     87        0.16054       0.24028       0.23143       0.012937      3      4      3.99          0.099499      0.19245       0.23577       0.22026       0.012342      3      4      3.9           0.3       
INFO:pipegenie_evolution:48     90        0.14661       0.24174       0.22695       0.020349      3      4      3.985         0.12155       0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:49     101       0.14892       0.23953       0.23072       0.013827      3      4      3.99          0.099499      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:50     109       0.16431       0.24174       0.23049       0.014901      3      4      3.99          0.099499      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:51     110       0.18493       0.24174       0.23184       0.011503      3      4      3.99          0.099499      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:52     123       0.14256       0.24174       0.22967       0.017649      3      4      3.99          0.099499      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:53     109       0.14082       0.24174       0.2293        0.019272      3      4      3.99          0.099499      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:54     102       0.16485       0.24212       0.23197       0.013107      3      4      3.995         0.070534      0.19245       0.24174       0.22069       0.012917      3      4      3.9           0.3       
INFO:pipegenie_evolution:55     114       0.15466       0.24212       0.23289       0.014747      3      4      3.995         0.070534      0.19245       0.24174       0.22165       0.013668      3      4      3.9           0.3       
INFO:pipegenie_evolution:56     91        0.16554       0.24212       0.2346        0.013671      3      4      3.99          0.099499      0.19245       0.24174       0.22165       0.013668      3      4      3.9           0.3       
INFO:pipegenie_evolution:57     89        0.14554       0.24212       0.23368       0.016686      3      4      3.995         0.070534      0.19245       0.24174       0.22165       0.013668      3      4      3.9           0.3       
INFO:pipegenie_evolution:58     86        0.16042       0.24212       0.23338       0.017002      3      4      3.99          0.099499      0.19245       0.24174       0.22165       0.013668      3      4      3.9           0.3       
INFO:pipegenie_evolution:59     87        0.16146       0.24212       0.23264       0.018407      3      4      3.995         0.070534      0.19245       0.24174       0.22165       0.013668      3      4      3.9           0.3       
INFO:pipegenie_evolution:60     86        0.16705       0.24212       0.23602       0.014019      3      4      3.995         0.070534      0.19245       0.24174       0.22227       0.013237      3      4      3.9           0.3       
INFO:pipegenie_evolution:61     77        0.15109       0.24212       0.23673       0.014547      3      4      3.99          0.099499      0.19245       0.24174       0.22227       0.013237      3      4      3.9           0.3       
INFO:pipegenie_evolution:62     72        0.16248       0.24212       0.2375        0.011739      3      4      3.995         0.070534      0.20483       0.24116       0.22318       0.01147       3      4      3.95          0.21794   
INFO:pipegenie_evolution:63     100       0.17628       0.24212       0.23669       0.013228      3      4      3.995         0.070534      0.20483       0.24116       0.22318       0.01147       3      4      3.95          0.21794   
INFO:pipegenie_evolution:64     86        0.16416       0.24212       0.23651       0.014561      3      4      3.995         0.070534      0.20483       0.24116       0.22318       0.01147       3      4      3.95          0.21794   
INFO:pipegenie_evolution:65     78        0.14823       0.24212       0.23402       0.020294      3      4      3.995         0.070534      0.20483       0.24116       0.22318       0.01147       3      4      3.95          0.21794   
INFO:pipegenie_evolution:66     76        0.16567       0.24212       0.23797       0.013408      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:67     85        0.166         0.24212       0.23722       0.013843      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:68     90        0.15306       0.24212       0.23545       0.018473      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:69     84        0.14466       0.24212       0.23698       0.016485      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:70     82        0.18792       0.24212       0.23828       0.011084      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:71     83        0.14663       0.24212       0.23519       0.020415      3      4      3.995         0.070534      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:72     83        0.15348       0.24212       0.2355        0.017942      3      4      3.99          0.099499      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:73     89        0.15399       0.24212       0.2351        0.018158      3      4      3.99          0.099499      0.20483       0.24116       0.22309       0.011339      3      4      3.95          0.21794   
INFO:pipegenie_evolution:74     80        0.16624       0.24212       0.23762       0.01336       3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
INFO:pipegenie_evolution:75     79        0.19687       0.24212       0.23946       0.0087822     3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
INFO:pipegenie_evolution:76     69        0.20259       0.24212       0.24025       0.0068135     3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
INFO:pipegenie_evolution:77     90        0.14827       0.24212       0.2371        0.016854      3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
INFO:pipegenie_evolution:78     80        0.18688       0.24212       0.23988       0.007995      3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
INFO:pipegenie_evolution:79     81        0.16057       0.24212       0.2378        0.014262      3      4      3.995         0.070534      0.20483       0.24203       0.22328       0.011623      3      4      3.95          0.21794   
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-11 23:44:54
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:01:04.104904
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2122
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-11 23:44:55
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2121.

‚ñ∂Ô∏è Processing OpenML Task ID: 2122 ---
   Dataset: kropt, Shape: 28056.0x7.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.055556      0.78393       0.42521       0.18521       3      4      3.74          0.43863       0.59297       0.78393       0.6844        0.060365      3      4      3.8           0.4       
INFO:pipegenie_evolution:1      151       0.085114      0.78393       0.46704       0.17908       3      4      3.77          0.42083       0.63137       0.78393       0.72174       0.040784      3      4      3.85          0.35707   
INFO:pipegenie_evolution:2      160       0.085114      0.78458       0.55096       0.15739       3      4      3.87          0.3363        0.73375       0.78458       0.75899       0.01589       3      4      3.9           0.3       
INFO:pipegenie_evolution:3      147       0.10562       0.79687       0.6102        0.12691       3      4      3.88          0.32496       0.75058       0.79687       0.77346       0.012507      3      4      3.95          0.21794   
INFO:pipegenie_evolution:4      159       0.080765      0.79687       0.65971       0.13028       3      4      3.87          0.3363        0.76161       0.79687       0.78204       0.009156      3      4      3.9           0.3       
INFO:pipegenie_evolution:5      135       0.14133       0.79808       0.70509       0.10159       3      4      3.89          0.31289       0.76161       0.79808       0.78693       0.0084184     3      4      3.9           0.3       
INFO:pipegenie_evolution:6      156       0.4846        0.79808       0.74369       0.054098      3      4      3.89          0.31289       0.76161       0.79808       0.78904       0.009128      3      4      3.9           0.3       
INFO:pipegenie_evolution:7      126       0.68139       0.8031        0.76907       0.025807      3      4      3.96          0.19596       0.76161       0.8031        0.79176       0.0092329     3      4      3.9           0.3       
INFO:pipegenie_evolution:8      130       0.72539       0.80942       0.782         0.015068      3      4      3.95          0.21794       0.76161       0.80942       0.79375       0.010269      3      4      3.85          0.35707   
INFO:pipegenie_evolution:9      140       0.64991       0.80942       0.78379       0.022836      3      4      3.94          0.23749       0.77882       0.80942       0.79838       0.0071211     3      4      3.85          0.35707   
INFO:pipegenie_evolution:10     137       0.62708       0.82116       0.78805       0.022981      3      4      3.95          0.21794       0.77882       0.82116       0.8003        0.0091192     3      4      3.85          0.35707   
INFO:pipegenie_evolution:11     148       0.61854       0.82116       0.79065       0.0205        3      4      3.945         0.22798       0.77882       0.82116       0.8053        0.010467      3      4      3.95          0.21794   
INFO:pipegenie_evolution:12     127       0.57769       0.82589       0.79034       0.032414      3      4      3.96          0.19596       0.77882       0.82589       0.8113        0.011593      3      4      3.9           0.3       
INFO:pipegenie_evolution:13     134       0.50507       0.82589       0.79256       0.043421      3      4      3.985         0.12155       0.77882       0.82589       0.81387       0.011736      3      4      3.95          0.21794   
INFO:pipegenie_evolution:14     150       0.54497       0.82589       0.79566       0.041427      3      4      3.99          0.099499      0.78393       0.82589       0.81823       0.008987      3      4      3.95          0.21794   
INFO:pipegenie_evolution:15     138       0.68315       0.82603       0.80472       0.019245      3      4      3.98          0.14          0.78393       0.82603       0.81871       0.0091815     3      4      3.95          0.21794   
INFO:pipegenie_evolution:16     120       0.092978      0.82603       0.80042       0.068711      3      4      3.96          0.19596       0.78393       0.82603       0.81929       0.0090599     4      4      4             0         
INFO:pipegenie_evolution:17     122       0.39422       0.82603       0.80506       0.058039      3      4      3.93          0.25515       0.78393       0.82603       0.81944       0.0090197     4      4      4             0         
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-12 00:49:47
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:04:52.563884
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2123
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 00:49:48
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2122.

‚ñ∂Ô∏è Processing OpenML Task ID: 2123 ---
   Dataset: baseball, Shape: 1340.0x17.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.33211       0.75566       0.60622       0.10338       3      4      3.745         0.43586       0.7133        0.75566       0.73453       0.015186      3      4      3.75          0.43301   
INFO:pipegenie_evolution:1      149       0.3272        0.76696       0.63294       0.087364      3      4      3.765         0.424         0.70873       0.76696       0.74489       0.013295      3      4      3.7           0.45826   
INFO:pipegenie_evolution:2      138       0.3272        0.76696       0.65196       0.085993      3      4      3.765         0.424         0.70873       0.76696       0.74518       0.01327       3      4      3.65          0.47697   
INFO:pipegenie_evolution:3      136       0.53197       0.78097       0.6931        0.054011      3      4      3.745         0.43586       0.72071       0.78097       0.75015       0.012543      3      4      3.7           0.45826   
INFO:pipegenie_evolution:4      137       0.507         0.78097       0.71786       0.044327      3      4      3.7           0.45826       0.72812       0.78097       0.75775       0.012054      3      4      3.8           0.4       
INFO:pipegenie_evolution:5      118       0.55753       0.78097       0.73107       0.036964      3      4      3.685         0.46452       0.73761       0.78097       0.75915       0.010374      3      4      3.85          0.35707   
INFO:pipegenie_evolution:6      116       0.58252       0.78097       0.74118       0.027428      3      4      3.69          0.46249       0.73761       0.78097       0.75913       0.010406      3      4      3.85          0.35707   
INFO:pipegenie_evolution:7      133       0.64787       0.78097       0.74987       0.018352      3      4      3.73          0.44396       0.72977       0.78097       0.75996       0.013516      3      4      3.8           0.4       
INFO:pipegenie_evolution:8      133       0.66668       0.78097       0.75418       0.015687      3      4      3.825         0.37997       0.72977       0.78097       0.76231       0.012625      3      4      3.85          0.35707   
INFO:pipegenie_evolution:9      111       0.63444       0.79041       0.75314       0.022682      3      4      3.83          0.37563       0.72977       0.79041       0.76507       0.014044      3      4      3.9           0.3       
INFO:pipegenie_evolution:10     118       0.63204       0.79041       0.75571       0.024038      3      4      3.885         0.31902       0.73761       0.79041       0.76768       0.011935      3      4      3.95          0.21794   
INFO:pipegenie_evolution:11     134       0.63614       0.79041       0.75859       0.025209      3      4      3.92          0.27129       0.74457       0.79041       0.77354       0.010214      3      4      3.95          0.21794   
INFO:pipegenie_evolution:12     140       0.62361       0.79041       0.75595       0.031462      3      4      3.97          0.17059       0.74457       0.79041       0.77354       0.010214      3      4      3.95          0.21794   
INFO:pipegenie_evolution:13     134       0.33333       0.79041       0.74595       0.072353      3      4      3.98          0.14          0.76696       0.79041       0.77761       0.0083674     4      4      4             0         
INFO:pipegenie_evolution:14     127       0.52555       0.79041       0.75725       0.046172      3      4      3.99          0.099499      0.74457       0.79041       0.77905       0.010641      3      4      3.95          0.21794   
INFO:pipegenie_evolution:15     120       0.57131       0.82129       0.76203       0.04235       3      4      3.99          0.099499      0.74457       0.82129       0.78271       0.013716      3      4      3.95          0.21794   
INFO:pipegenie_evolution:16     128       0.62416       0.82129       0.76404       0.038422      3      4      3.99          0.099499      0.74457       0.82129       0.78154       0.015925      3      4      3.9           0.3       
INFO:pipegenie_evolution:17     126       0.59535       0.82129       0.76656       0.03951       3      4      3.985         0.12155       0.74457       0.82129       0.78592       0.018312      3      4      3.9           0.3       
INFO:pipegenie_evolution:18     134       0.63683       0.82129       0.76972       0.032743      3      4      3.985         0.12155       0.74457       0.82129       0.789         0.016142      3      4      3.95          0.21794   
INFO:pipegenie_evolution:19     132       0.57758       0.82129       0.77112       0.037263      3      4      3.99          0.099499      0.74457       0.82129       0.79168       0.015542      3      4      3.95          0.21794   
INFO:pipegenie_evolution:20     144       0.52793       0.82129       0.77133       0.042387      3      4      3.985         0.12155       0.78097       0.82129       0.79775       0.011466      4      4      4             0         
INFO:pipegenie_evolution:21     108       0.58061       0.82129       0.77361       0.041691      4      4      4             0             0.78097       0.82129       0.8026        0.011637      4      4      4             0         
INFO:pipegenie_evolution:22     125       0.61761       0.82129       0.78017       0.034767      4      4      4             0             0.78097       0.82129       0.80427       0.01188       4      4      4             0         
INFO:pipegenie_evolution:23     137       0.50742       0.82129       0.7726        0.051433      4      4      4             0             0.78097       0.82129       0.80513       0.01135       4      4      4             0         
INFO:pipegenie_evolution:24     128       0.55093       0.82129       0.77384       0.052833      3      4      3.995         0.070534      0.78097       0.82129       0.80577       0.011561      4      4      4             0         
INFO:pipegenie_evolution:25     135       0.64939       0.82129       0.79032       0.026866      4      4      4             0             0.78097       0.82129       0.80577       0.011561      4      4      4             0         
INFO:pipegenie_evolution:26     117       0.33252       0.82129       0.7871        0.048546      3      4      3.995         0.070534      0.78097       0.82129       0.80577       0.011561      4      4      4             0         
INFO:pipegenie_evolution:27     108       0.58504       0.82129       0.7854        0.04485       4      4      4             0             0.78097       0.82129       0.80721       0.011827      4      4      4             0         
INFO:pipegenie_evolution:28     125       0.61075       0.82129       0.78452       0.047341      4      4      4             0             0.78097       0.82129       0.80792       0.012147      4      4      4             0         
INFO:pipegenie_evolution:29     122       0.66234       0.82129       0.7913        0.0313        4      4      4             0             0.78097       0.82129       0.80946       0.010473      4      4      4             0         
INFO:pipegenie_evolution:30     119       0.61553       0.82129       0.7931        0.029943      4      4      4             0             0.78097       0.82129       0.81195       0.010198      4      4      4             0         
INFO:pipegenie_evolution:31     111       0.66104       0.82129       0.79305       0.03211       4      4      4             0             0.79041       0.82129       0.81467       0.007385      4      4      4             0         
INFO:pipegenie_evolution:32     126       0.58828       0.82129       0.7885        0.050249      3      4      3.995         0.070534      0.79041       0.82129       0.81471       0.0073706     4      4      4             0         
INFO:pipegenie_evolution:33     119       0.62896       0.82129       0.79396       0.037294      4      4      4             0             0.80481       0.82129       0.81671       0.0048778     4      4      4             0         
INFO:pipegenie_evolution:34     124       0.6046        0.82129       0.79534       0.040029      4      4      4             0             0.80481       0.82129       0.81721       0.0048174     4      4      4             0         
INFO:pipegenie_evolution:35     136       0.62155       0.82129       0.79092       0.045744      4      4      4             0             0.80481       0.82129       0.81755       0.0047374     4      4      4             0         
INFO:pipegenie_evolution:36     125       0.33333       0.82129       0.78214       0.071137      3      4      3.995         0.070534      0.81193       0.82129       0.81876       0.0034939     4      4      4             0         
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-12 01:50:44
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:56.744854
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2125
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 01:50:45
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2123.

‚ñ∂Ô∏è Processing OpenML Task ID: 2125 ---
   Dataset: eucalyptus, Shape: 736.0x20.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.20286       0.63613       0.51907       0.084789      3      4      3.76          0.42708       0.59885       0.63613       0.61098       0.0092442     3      4      3.9           0.3       
INFO:pipegenie_evolution:1      149       0.20833       0.63613       0.54572       0.069387      3      4      3.785         0.41082       0.59096       0.63613       0.61457       0.011336      3      4      3.85          0.35707   
INFO:pipegenie_evolution:2      129       0.30636       0.64733       0.57192       0.053074      3      4      3.775         0.41758       0.5888        0.64733       0.6201        0.013618      3      4      3.85          0.35707   
INFO:pipegenie_evolution:3      139       0.43958       0.64733       0.58957       0.036303      3      4      3.825         0.37997       0.61739       0.64733       0.62958       0.0060595     3      4      3.95          0.21794   
INFO:pipegenie_evolution:4      134       0.51894       0.64733       0.60328       0.021701      3      4      3.8           0.4           0.60593       0.64733       0.62975       0.0076127     4      4      4             0         
INFO:pipegenie_evolution:5      122       0.56638       0.64733       0.60973       0.016945      3      4      3.855         0.3521        0.61739       0.64733       0.63099       0.0059636     3      4      3.95          0.21794   
INFO:pipegenie_evolution:6      122       0.53898       0.64733       0.61419       0.016905      3      4      3.86          0.34699       0.62496       0.64733       0.63222       0.0050967     3      4      3.95          0.21794   
INFO:pipegenie_evolution:7      112       0.52642       0.64733       0.62003       0.014963      3      4      3.91          0.28618       0.62496       0.64733       0.63316       0.0060453     3      4      3.9           0.3       
INFO:pipegenie_evolution:8      108       0.41957       0.64733       0.62323       0.020726      3      4      3.94          0.23749       0.62496       0.64733       0.63401       0.0060492     3      4      3.9           0.3       
INFO:pipegenie_evolution:9      124       0.49283       0.64805       0.62847       0.013897      3      4      3.945         0.22798       0.62496       0.64805       0.63567       0.006724      3      4      3.9           0.3       
INFO:pipegenie_evolution:10     138       0.44594       0.64805       0.62634       0.024131      3      4      3.925         0.26339       0.62496       0.64805       0.63612       0.0071498     3      4      3.95          0.21794   
INFO:pipegenie_evolution:11     127       0.5395        0.64813       0.6306        0.014389      3      4      3.915         0.27888       0.62496       0.64813       0.63749       0.0079419     3      4      3.95          0.21794   
INFO:pipegenie_evolution:12     121       0.58038       0.65157       0.6329        0.012913      3      4      3.955         0.2073        0.62496       0.65157       0.63956       0.0085303     3      4      3.95          0.21794   
INFO:pipegenie_evolution:13     118       0.59948       0.65157       0.63647       0.0099799     3      4      3.975         0.15612       0.62496       0.65157       0.64019       0.008941      3      4      3.95          0.21794   
INFO:pipegenie_evolution:14     111       0.58025       0.65157       0.63758       0.013072      3      4      3.985         0.12155       0.62496       0.65157       0.64019       0.008941      3      4      3.95          0.21794   
INFO:pipegenie_evolution:15     118       0.55023       0.65304       0.6401        0.013394      3      4      3.98          0.14          0.62496       0.65304       0.64051       0.0092689     3      4      3.95          0.21794   
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-12 02:51:39
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:00:54.184680
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: balanced_accuracy_score, Maximization: True
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_2356
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 02:51:39
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2125.

‚ñ∂Ô∏è Processing OpenML Task ID: 2356 ---
   Dataset: meta_stream_intervals.arff, Shape: 45164.0x75.0
   Target variable is categorical. Applying LabelEncoder.
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.090909      0.97591       0.73823       0.23548       3      4      3.75          0.43301       0.90502       0.97591       0.9441        0.020101      3      4      3.8           0.4       
INFO:pipegenie_evolution:1      179       0.090909      0.97591       0.79515       0.20649       3      4      3.83          0.37563       0.92615       0.97591       0.95555       0.011542      3      4      3.8           0.4       
INFO:pipegenie_evolution:2      172       0.090909      0.9792        0.83063       0.19923       3      4      3.805         0.3962        0.94613       0.9792        0.96378       0.0091118     3      4      3.55          0.49749   
INFO:pipegenie_evolution:3      171       0.090909      0.9792        0.84843       0.19246       3      4      3.805         0.3962        0.96021       0.9792        0.96891       0.0058484     3      4      3.6           0.4899    
INFO:pipegenie_evolution:4      175       0.57645       0.9792        0.90005       0.096347      3      4      3.79          0.40731       0.96346       0.9792        0.97093       0.004304      3      4      3.65          0.47697   
INFO:pipegenie_evolution:5      167       0.10586       0.9792        0.89791       0.13262       3      4      3.76          0.42708       0.96653       0.9792        0.97199       0.0036001     3      4      3.7           0.45826   
INFO:pipegenie_evolution:6      169       0.090909      0.9792        0.89154       0.15461       3      4      3.785         0.41082       0.96653       0.9792        0.97265       0.0034252     3      4      3.6           0.4899    
WARNING:pipegenie_evolution:Timeout of 3600s reached. Stopping evolution.
INFO:pipegenie_evolution:[3/5] ‚úÖ Evolution finished.
INFO:pipegenie_evolution:    - Saving run results to meta-knowledge base...
INFO:pipegenie_evolution:[4/5] üëë Generating and evaluating final ensemble...
INFO:pipegenie_evolution:    - Ensemble models and weights saved to `ensemble.txt`.
INFO:pipegenie_evolution:[5/5] ‚ú® PipeGenie run complete.
INFO:pipegenie_evolution:üïí End Time: 2025-07-12 03:56:16
INFO:pipegenie_evolution:‚è±Ô∏è Total Elapsed Time: 1:04:36.521796
INFO:pipegenie_evolution:--- End of Run ---
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_359997
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:56:16
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚úÖ Successfully finished and saved task 2356.

‚ñ∂Ô∏è Processing OpenML Task ID: 359997 ---
   Dataset: fri_c4_500_25, Shape: 500.0x26.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.44979       1.1525        0.65817       0.17861       3      4      3.195         0.3962        0.44979       0.48173       0.46772       0.008856      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_359998
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:56:50
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 359997: 

‚ñ∂Ô∏è Processing OpenML Task ID: 359998 ---
   Dataset: fri_c3_500_10, Shape: 500.0x11.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.37611       1.0433        0.60231       0.17043       3      4      3.195         0.3962        0.37611       0.43057       0.41462       0.013757      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360000
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:57:11
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 359998: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360000 ---
   Dataset: fri_c1_500_10, Shape: 500.0x11.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.37811       1.019         0.59486       0.17871       3      4      3.195         0.3962        0.37811       0.42105       0.40321       0.011734      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360001
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:57:32
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360000: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360001 ---
   Dataset: fri_c4_500_100, Shape: 500.0x101.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.50828       1.2706        0.73782       0.21851       3      4      3.185         0.3883        0.50828       0.54335       0.52765       0.01152       3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360003
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:58:50
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360001: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360003 ---
   Dataset: pm10, Shape: 500.0x8.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       0.71584       1.462e+06     10014         1.2058e+05    3      4      3.195         0.3962        0.71584       0.7391        0.72999       0.0063781     3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_167146
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:59:11
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360003: 

‚ñ∂Ô∏è Processing OpenML Task ID: 167146 ---
   Dataset: boston, Shape: 506.0x14.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       3.5732        9.3118e+12    8.9368e+10    8.2548e+11    3      4      3.195         0.3962        3.5732        3.8015        3.7284        0.060831      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360004
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 03:59:47
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 167146: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360004 ---
   Dataset: boston_corrected, Shape: 506.0x21.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       3.4078        3.1008e+96    2.1094e+94    2.5488e+95    3      4      3.185         0.3883        3.4078        3.5397        3.4882        0.041514      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 98, in run_openml_task
    X_train_processed = preprocessor.fit_transform(X_train_df)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/utils/_set_output.py", line 319, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py", line 1001, in fit_transform
    result = self._call_func_on_transformers(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py", line 910, in _call_func_on_transformers
    return Parallel(n_jobs=self.n_jobs)(jobs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/utils/parallel.py", line 77, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/joblib/parallel.py", line 1986, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/joblib/parallel.py", line 1914, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/utils/parallel.py", line 139, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/pipeline.py", line 1551, in _fit_transform_one
    res = transformer.fit_transform(X, y, **params.get("fit_transform", {}))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/pipeline.py", line 730, in fit_transform
    return last_step.fit_transform(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/utils/_set_output.py", line 319, in wrapped
    data_to_wrap = f(self, X, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/base.py", line 918, in fit_transform
    return self.fit(X, **fit_params).transform(X)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/preprocessing/_data.py", line 894, in fit
    return self.partial_fit(X, y, sample_weight)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/base.py", line 1389, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie-env/lib/python3.12/site-packages/sklearn/preprocessing/_data.py", line 959, in partial_fit
    raise ValueError(
ValueError: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360006
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:01:05
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360004: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360005 ---
   Dataset: QSAR-TID-11626, Shape: 507.0x1026.0
‚ùå Failed to process task 360005: Cannot center sparse matrices: pass `with_mean=False` instead. See docstring for motivation and alternatives.

‚ñ∂Ô∏è Processing OpenML Task ID: 360006 ---
   Dataset: rmftsa_ladata, Shape: 508.0x11.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       1.8212        4.2852e+49    2.9151e+47    3.5224e+48    3      4      3.195         0.3962        1.8212        1.9042        1.8515        0.035835      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360007
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:01:27
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360006: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360007 ---
   Dataset: forest_fires, Shape: 517.0x13.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       27.821        1.7388e+10    1.1829e+08    1.4293e+09    3      4      3.195         0.3962        27.821        29.034        28.608        0.36941       3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_211696
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:02:01
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360007: 

‚ñ∂Ô∏è Processing OpenML Task ID: 211696 ---
   Dataset: meta, Shape: 528.0x22.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       187.98        1.1578e+15    1.0532e+13    1.0022e+14    3      4      3.185         0.3883        187.98        197.74        194.88        2.5399        3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360009
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:02:47
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 211696: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360009 ---
   Dataset: cps_85_wages, Shape: 534.0x11.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       4.5027        6.9766e+98    4.746e+96     5.7346e+97    3      4      3.195         0.3962        4.5027        4.5846        4.5504        0.016667      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360010
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:03:20
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360009: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360010 ---
   Dataset: arsenic-female-lung, Shape: 559.0x5.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       3.3995        3.6476e+08    2.4983e+06    3.0084e+07    3      4      3.195         0.3962        3.3995        4.7339        3.9638        0.62541       3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360011
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:04:06
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360010: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360011 ---
   Dataset: arsenic-male-bladder, Shape: 559.0x5.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       2.6951        1.28e+09      8.8274e+06    1.0593e+08    3      4      3.195         0.3962        2.6951        2.9713        2.8988        0.070273      3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
INFO:pipegenie_evolution:[1/5] ‚öôÔ∏è  Configuration
INFO:pipegenie_evolution:    - Generations: 200, Population Size: 200
INFO:pipegenie_evolution:    - Elite Size: 20, Timeout: 3600s
INFO:pipegenie_evolution:    - Fitness Function: root_mean_squared_error, Maximization: False
INFO:pipegenie_evolution:    - Output Directory: /hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/temp_results/task_360012
INFO:pipegenie_evolution:    - Random Seed: 42
INFO:pipegenie_evolution:--- PipeGenie Run Started ---
INFO:pipegenie_evolution:üïí Start Time: 2025-07-12 04:04:47
INFO:pipegenie_evolution:[2/5] üå± Initializing evolutionary process...
INFO:pipegenie_evolution:    - Using meta-learning to warm-start population...
INFO:pipegenie_evolution:    - Seeding population with 50 pipeline(s) from past runs.
‚ùå Failed to process task 360011: 

‚ñ∂Ô∏è Processing OpenML Task ID: 360012 ---
   Dataset: arsenic-male-lung, Shape: 559.0x5.0
   Preprocessing complete.
INFO:pipegenie_evolution:                                       fitness                                            size                                        fitness_elite                                      size_elite              
                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------
gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       
0      200       12.291        5.272e+11     3.6359e+09    4.3631e+10    3      4      3.195         0.3962        12.291        13.302        13.146        0.27724       3      3      3             0         
Traceback (most recent call last):
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/build_metabase.py", line 122, in run_openml_task
    model.fit(X_train_processed, y_train_np)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/regression/pipegenie_regressor.py", line 233, in fit
    return super().fit(X=X, y=y, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/_classification_regression.py", line 184, in fit
    self._fit(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 482, in _fit
    self._evolve(data)
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 617, in _evolve
    offspring = apply_operators(population)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/base.py", line 746, in _apply_operators_cx_mut
    offspring[i], modified = self.mutation.mutate(offspring[i], self.schema)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 76, in mutate
    ind = self._apply_mutation(ind, schema)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 115, in _apply_mutation
    return self.hyper_mutation._apply_mutation(ind, schema)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/evolutionary/mutation.py", line 209, in _apply_mutation
    parent_idx = next(idx for idx, node in enumerate(reversed(ind[:len(ind) - last_alg_idx]))
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration
‚ùå Failed to process task 360012: 

Meta-Base Build Process Finished.
Knowledge base is located in: '/hhome/bpicot25/autoML_uco/evoflow-dev-main/pipegenie/metalearning/openml_metabase'
Exception in thread Thread-29 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-28 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-27 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-26 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-25 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-24 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-23 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-22 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-21 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-20 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-19 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-18 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-17 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-16 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-15 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-14 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-13 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-12 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-11 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-10 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-8 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-7 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-6 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-5 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-4 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-3 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
Exception in thread Thread-2 (_monitor):
Traceback (most recent call last):
  File "/usr/lib/python3.12/threading.py", line 1073, in _bootstrap_inner
    self.run()
  File "/usr/lib/python3.12/threading.py", line 1010, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/lib/python3.12/logging/handlers.py", line 1574, in _monitor
    record = self.dequeue(True)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/logging/handlers.py", line 1523, in dequeue
    return self.queue.get(block)
           ^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 2, in get
  File "/usr/lib/python3.12/multiprocessing/managers.py", line 821, in _callmethod
    kind, result = conn.recv()
                   ^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
          ^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 430, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/usr/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError
