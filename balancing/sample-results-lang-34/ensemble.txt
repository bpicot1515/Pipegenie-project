0: Pipeline(
	0: MinMaxScaler()
	1: VarianceThreshold()
	2: FastICA(n_components=17, random_state=34, whiten='arbitrary-variance')
	3: DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
                       max_depth=28, max_features=0.5253960128423623,
                       min_samples_leaf=18, min_samples_split=7,
                       random_state=34)
) -> Fitness: 0.8015486018562077

1: Pipeline(
	0: RandomForestClassifier(class_weight='balanced', max_features=0.5925861149273224,
                       min_samples_leaf=18, min_samples_split=6,
                       n_estimators=53, random_state=34)
) -> Fitness: 0.8054638236351928

2: Pipeline(
	0: DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
                       max_depth=28, max_features=0.5253960128423623,
                       min_samples_leaf=18, min_samples_split=7,
                       random_state=34)
) -> Fitness: 0.7933240350792494

3: Pipeline(
	0: ExtraTreesClassifier(class_weight='balanced', max_features='log2',
                     min_samples_leaf=4, min_samples_split=14, n_estimators=21,
                     random_state=34)
) -> Fitness: 0.8034345531280419

4: Pipeline(
	0: MaxAbsScaler()
	1: RandomForestClassifier(bootstrap=False, class_weight='balanced',
                       max_features=0.9077169560752465, min_samples_leaf=20,
                       min_samples_split=3, n_estimators=69, random_state=34)
) -> Fitness: 0.7934046486405164

5: Pipeline(
	0: MaxAbsScaler()
	1: MinMaxScaler()
	2: ExtraTreesClassifier(class_weight='balanced', max_features='log2',
                     min_samples_leaf=4, min_samples_split=14, n_estimators=21,
                     random_state=34)
) -> Fitness: 0.8034345531280419

6: Pipeline(
	0: MinMaxScaler()
	1: MaxAbsScaler()
	2: DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
                       max_depth=28, max_features=0.5253960128423623,
                       min_samples_leaf=18, min_samples_split=7,
                       random_state=34)
) -> Fitness: 0.7933240350792494

7: Pipeline(
	0: MaxAbsScaler()
	1: RandomForestClassifier(bootstrap=False, class_weight='balanced',
                       max_features=0.9395071589118962, min_samples_leaf=20,
                       min_samples_split=3, n_estimators=69, random_state=34)
) -> Fitness: 0.7934046486405164

8: Pipeline(
	0: VarianceThreshold()
	1: RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',
                       max_features=0.5627906444557076, min_samples_leaf=16,
                       min_samples_split=12, n_estimators=26, random_state=34)
) -> Fitness: 0.7899436547134794

9: Pipeline(
	0: MinMaxScaler()
	1: AdaBoostClassifier(algorithm='SAMME',
                   estimator=DecisionTreeClassifier(class_weight='balanced',
                                                    max_depth=5,
                                                    max_features=0.46325108773124835,
                                                    min_samples_leaf=20,
                                                    min_samples_split=11,
                                                    random_state=34),
                   learning_rate=1.0426649350480794, n_estimators=31,
                   random_state=34)
) -> Fitness: 0.7860736143281061

10: Pipeline(
	0: MaxAbsScaler()
	1: RandomForestClassifier(class_weight='balanced_subsample', criterion='entropy',
                       max_features=0.5627906444557076, min_samples_leaf=16,
                       min_samples_split=12, n_estimators=26, random_state=34)
) -> Fitness: 0.7897524501245693

11: Pipeline(
	0: MaxAbsScaler()
	1: DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
                       max_depth=16, max_features=0.7039310743163678,
                       min_samples_leaf=16, min_samples_split=8,
                       random_state=34)
) -> Fitness: 0.7747061572069235

12: Pipeline(
	0: MinMaxScaler()
	1: RobustScaler(with_centering=False, with_scaling=False)
	2: ExtraTreesClassifier(bootstrap=True, class_weight='balanced',
                     min_samples_leaf=11, n_estimators=27, random_state=34)
) -> Fitness: 0.7842622231154335

13: Pipeline(
	0: RandomForestClassifier(bootstrap=False, class_weight='balanced',
                       max_features=0.549482318144271, min_samples_leaf=9,
                       min_samples_split=11, random_state=34)
) -> Fitness: 0.7777428352545483

14: Pipeline(
	0: AdaBoostClassifier(algorithm='SAMME',
                   estimator=DecisionTreeClassifier(class_weight='balanced',
                                                    criterion='entropy',
                                                    max_depth=3,
                                                    max_features=0.9947454811148053,
                                                    min_samples_leaf=3,
                                                    min_samples_split=11,
                                                    random_state=34),
                   learning_rate=0.28647243853835647, n_estimators=32,
                   random_state=34)
) -> Fitness: 0.7723502782739716

15: Pipeline(
	0: DecisionTreeClassifier(class_weight='balanced', max_depth=13,
                       max_features=0.04292777608217091, min_samples_leaf=20,
                       min_samples_split=15, random_state=34)
) -> Fitness: 0.7516875423604803

16: Pipeline(
	0: MinMaxScaler()
	1: Nystroem(coef0=-0.18705612470607158, degree=3, gamma=4.534558559389873,
         kernel='cosine', n_components=26, random_state=34)
	2: MaxAbsScaler()
	3: VarianceThreshold()
	4: RandomForestClassifier(bootstrap=False, class_weight='balanced',
                       max_features=0.9077169560752465, min_samples_leaf=20,
                       min_samples_split=3, n_estimators=69, random_state=34)
) -> Fitness: 0.7656089505005039

17: Pipeline(
	0: SelectFwe(alpha=0.025064203667582212)
	1: LogisticRegression(C=3.2649258645047388, class_weight='balanced', max_iter=1000,
                   penalty='l1', random_state=34, solver='liblinear')
) -> Fitness: 0.7138003287416538

18: Pipeline(
	0: MinMaxScaler()
	1: VarianceThreshold()
	2: FastICA(n_components=17, random_state=34, whiten='arbitrary-variance')
	3: ExtraTreesClassifier(bootstrap=True, class_weight='balanced_subsample',
                     criterion='entropy', max_features='log2',
                     min_samples_leaf=10, min_samples_split=7, n_estimators=97,
                     random_state=34)
) -> Fitness: 0.7749946949989278

19: Pipeline(
	0: VarianceThreshold()
	1: MinMaxScaler()
	2: LogisticRegression(C=1.9587451911127762, class_weight='balanced', max_iter=1000,
                   penalty=None, random_state=34, solver='saga')
) -> Fitness: 0.6878244518071996

Ensemble fitness: 0.8375861665033064
Weights: [0.9951391711656087, 1.0, 0.9849282013670652, 0.9974806186850299, 0.9850282847710636, 0.9974806186850299, 0.9849282013670652, 0.9850282847710636, 0.9807313892116616, 0.9759266540121251, 0.9804940047590028, 0.9618137208329794, 0.9736777743486071, 0.9655838194500965, 0.9588888484006964, 0.9332356343056959, 0.9505193505093535, 0.8861978748097633, 0.9621719464708512, 0.8539482862221335]
Prediction: [0 0 0 ... 0 0 0]
