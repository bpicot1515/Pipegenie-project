{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "This tutorial shows how to use the `PipegenieClassifier` class. This class is used to create a model that can be used for classification tasks.\n",
    "\n",
    "First of all, we need to load the dataset. For this tutorial, we will use the `iris` dataset from the `sklearn` package, which is a simple dataset with 150 samples, 4 features and 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "print(iris.data.head())\n",
    "print(iris.target_names)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the generalization of the model, we will split the dataset into training and test sets. We will use 75% of the data for training and 25% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipegenie.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data, we can create a model using the `PipegenieClassifier` class. The `PipegenieClassifier` requires the path to the grammar file as a parameter, but it also accepts other parameters such as the number of generations, the population size, operators to apply, seed for reproducibility, etc. To see all the parameters available, you can check the documentation of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipegenie.classification import PipegenieClassifier\n",
    "from pipegenie.evolutionary.crossover import MultiCrossover\n",
    "from pipegenie.evolutionary.mutation import MultiMutation\n",
    "from pipegenie.evolutionary.replacement import ElitistGenerationalReplacement\n",
    "from pipegenie.evolutionary.selection import TournamentSelection\n",
    "\n",
    "model = PipegenieClassifier(\n",
    "    #Check the folder `/tutorials/` to access grammar XML files\n",
    "    grammar=\"sample-grammar-classification.xml\",\n",
    "    generations=5,\n",
    "    pop_size=50,\n",
    "    elite_size=5,\n",
    "    crossover=MultiCrossover(0.8),\n",
    "    mutation=MultiMutation(0.5),\n",
    "    selection=TournamentSelection(3),\n",
    "    replacement=ElitistGenerationalReplacement(5),\n",
    "    n_jobs=5,\n",
    "    seed=42,\n",
    "    outdir=\"sample-results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model created, we can now train it using the `fit` method passing the data features and labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       fitness                                            size                                        fitness_elite                                      size_elite              \n",
      "                 ----------------------------------------------------    --------------------------------------    ----------------------------------------------------    --------------------------------------\n",
      "gen    nevals    min           max           avg           std           min    max    avg           std           min           max           avg           std           min    max    avg           std       \n",
      "0      50        0.33333       0.97143       0.88701       0.14218       1      5      2.64          1.3381        0.95238       0.97143       0.95619       0.007619      2      3      2.2           0.4       \n",
      "1      45        0.33333       0.97143       0.91138       0.10502       1      5      2.7021        1.2702        0.95238       0.97143       0.9581        0.007619      1      3      2             0.63246   \n",
      "2      38        0.33333       0.97143       0.92632       0.098798      1      5      2.4651        1.1684        0.95238       0.97143       0.9619        0.0060234     1      2      1.8           0.4       \n",
      "3      40        0.91759       0.97143       0.9448        0.014293      1      4      2.0732        0.7454        0.9619        0.97143       0.96381       0.0038095     1      2      1.8           0.4       \n",
      "4      33        0.92381       0.97143       0.94737       0.013243      1      4      2.0263        0.62773       0.9619        0.97143       0.96381       0.0038095     1      2      1.8           0.4       \n",
      "5      30        0.92381       0.97143       0.95317       0.010618      1      4      2.25          0.86201       0.9619        0.97143       0.96571       0.0046657     2      2      2             0         \n",
      "--- 76.57577109336853 sec ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pipegenie.classification.pipegenie_classifier.PipegenieClassifier at 0x7345ca5118a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can evaluate the model's performance either using the `score` method or any other metric. The `score` method returns the accuracy of the model on the given data features and labels. Alternatively, you can use the `predict` method to get the predicted labels for the given data features or use the `predict_proba` method to get the probabilities of each class. You can then use these predictions to calculate other metrics provided by the `pipegenie` package, `sklearn` package or any other metric you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model score: 1.0\n",
      "Accuracy: 1.0\n",
      "Balanced accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model score: {model.score(X_test, y_test)}\")\n",
    "\n",
    "from pipegenie.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\") # Should be the same as model.score(X_test, y_test)\n",
    "print(f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [ensemble.txt](sample-results/ensemble.txt) output file, you can find the pipelines contained in the final ensemble model. Something to keep in mind is that the pipelines may not be completely sorted based on their fitness. This is because of the diversity weight, which aims to benefit pipelines with different predictions from the others in the ensemble. This can be useful to improve the generalization of the model in difficult datasets, but in simpler datasets, like the `iris` dataset, it may not be necessary. It can even occur that pipelines with lower fitness, that is, wrong predictions and, therefore, different from the others, are included in the ensemble model, decreasing the overall performance. In this case, you can try to decrease the diversity weight parameter or even set it to zero to avoid this behavior.\n",
    "\n",
    "On the other hand, in the [best_pipeline.txt](sample-results/best_pipeline.txt) output file, you can find the best pipeline found during the search process. This pipeline is the one with the highest fitness value, which must appear in the ensemble model, but not necessarily in the first position as mentioned before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipegenie-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
